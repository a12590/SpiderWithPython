/*
Navicat MySQL Data Transfer

Source Server         : master_host
Source Server Version : 50723
Source Host           : 10.108.211.136:3306
Source Database       : spark_mysql

Target Server Type    : MYSQL
Target Server Version : 50723
File Encoding         : 65001

Date: 2018-12-20 22:05:29
*/

SET FOREIGN_KEY_CHECKS=0;

-- ----------------------------
-- Table structure for algo
-- ----------------------------
DROP TABLE IF EXISTS `algo`;
CREATE TABLE `algo` (
  `id` int(50) NOT NULL,
  `algoPara` varchar(250) NOT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of algo
-- ----------------------------
INSERT INTO `algo` VALUES ('1', '{u\'method\':\'kendall\'}');
INSERT INTO `algo` VALUES ('2', '{u\'minsupport\': 0.75}');
INSERT INTO `algo` VALUES ('3', '{u\'max_depth\': 5, u\'min_samples_split\': 2,u\'min_samples_leaf\':1}');
INSERT INTO `algo` VALUES ('4', '{u\'C\': 1.0,u\'kernel\': \'linear\',u\'degree\': 3,u\'coef0\': 0.0}');
INSERT INTO `algo` VALUES ('5', '{u\'alpha\': 1.0}');
INSERT INTO `algo` VALUES ('6', '{u\'learning_rate\':1.2,u\'n_estimators\':425,u\'random_state\':1}');
INSERT INTO `algo` VALUES ('7', '{u\'tol\': 0.0002, u\'C\': 1.0, u\'max_iter\': 100, u\'n_jobs\': 1}');
INSERT INTO `algo` VALUES ('8', '{u\'n_estimators\': 100, u\'min_samples_split\': 2, u\'min_samples_leaf\': 1,u\'learning_rate\': 0.1,u\'max_depth\':3}');
INSERT INTO `algo` VALUES ('9', '{u\'n_jobs\': 1}');
INSERT INTO `algo` VALUES ('10', '{u\'alpha\': 1.0,u\'max_iter\': 50,u\'tol\':0.0002}');

-- ----------------------------
-- Table structure for datafile
-- ----------------------------
DROP TABLE IF EXISTS `datafile`;
CREATE TABLE `datafile` (
  `id` int(50) NOT NULL AUTO_INCREMENT,
  `jobId` int(50) DEFAULT NULL,
  `userid` int(50) DEFAULT NULL,
  `filename` text,
  `isdeleted` int(5) DEFAULT '0',
  `deletedon` datetime(6) DEFAULT NULL,
  `modifiedon` datetime(6) DEFAULT NULL,
  `createdon` datetime(6) DEFAULT NULL,
  `path` text,
  `separate` varchar(10) DEFAULT NULL,
  `firstLine` varchar(5) DEFAULT NULL,
  `label` varchar(5) DEFAULT NULL,
  `cat_list` varchar(100) DEFAULT NULL,
  `num_list` varchar(100) DEFAULT NULL,
  `weights` varchar(10) DEFAULT NULL,
  `datasetName` varchar(100) DEFAULT NULL,
  `firstStatus` int(10) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=77 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of datafile
-- ----------------------------
INSERT INTO `datafile` VALUES ('1', null, '4', '丈夫属性字段.csv', '0', null, null, '2018-07-19 18:34:15.549311', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', null, null, null, null, 'c', '1');
INSERT INTO `datafile` VALUES ('2', '1', '4', '丈夫属性字段.csv', '0', null, null, '2018-07-19 18:34:44.790923', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', null, null, null, null, 'c', '-1');
INSERT INTO `datafile` VALUES ('3', '2', '4', '丈夫属性字段.csv', '0', null, null, '2018-07-19 18:36:20.414506', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', null, null, null, null, 'c', '-1');
INSERT INTO `datafile` VALUES ('4', null, '4', 'churn.csv', '0', null, null, '2018-07-19 18:46:14.385694', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', ',', '1', null, null, null, null, '分类数据', '1');
INSERT INTO `datafile` VALUES ('5', '4', '4', 'churn.csv', '0', null, null, '2018-07-19 18:49:18.045739', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', ',', '1', '1', null, null, '0.2', '分类数据', '-1');
INSERT INTO `datafile` VALUES ('6', '5', '4', 'churn.csv', '0', null, null, '2018-07-19 19:28:35.967231', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', ',', '1', '1', null, null, '0.2', '分类数据', '-1');
INSERT INTO `datafile` VALUES ('7', '6', '4', 'churn.csv', '0', null, null, '2018-07-19 19:31:18.838494', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', ',', '1', '1', null, null, '0.2', '分类数据', '-1');
INSERT INTO `datafile` VALUES ('8', '7', '4', 'churn.csv', '0', null, null, '2018-07-19 19:33:25.130114', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', ',', '1', '1', null, null, '0.2', '分类数据', '-1');
INSERT INTO `datafile` VALUES ('9', '8', '4', 'churn.csv', '0', null, null, '2018-07-19 19:35:50.377191', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', ',', '1', '1', null, null, '0.3', '分类数据', '-1');
INSERT INTO `datafile` VALUES ('10', null, '4', 'hour_noheader.csv', '0', null, null, '2018-07-19 19:39:34.406438', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', ',', '0', null, null, null, null, '回归数据', '1');
INSERT INTO `datafile` VALUES ('11', '9', '4', 'hour_noheader.csv', '0', null, null, '2018-07-19 19:40:01.115922', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', ',', '0', '1', null, null, '0.3', '回归数据', '-1');
INSERT INTO `datafile` VALUES ('12', '10', '4', 'hour_noheader.csv', '0', null, null, '2018-07-19 19:46:18.396380', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', ',', '0', '1', null, null, '0.2', '回归数据', '-1');
INSERT INTO `datafile` VALUES ('13', '11', '4', 'hour_noheader.csv', '0', null, null, '2018-07-19 19:49:12.241992', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', ',', '0', null, null, null, null, '回归数据', '-1');
INSERT INTO `datafile` VALUES ('14', '14', '4', 'hour_noheader.csv', '0', null, null, '2018-07-19 19:55:05.981739', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', ',', '0', '1', null, null, '0.3', '回归数据', '-1');
INSERT INTO `datafile` VALUES ('15', '15', '4', 'hour_noheader.csv', '0', null, null, '2018-07-19 19:56:56.508756', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', ',', '0', '1', null, null, '0.3', '回归数据', '-1');
INSERT INTO `datafile` VALUES ('16', '16', '4', 'hour_noheader.csv', '0', null, null, '2018-07-19 19:59:24.532716', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', ',', '0', '1', null, null, '0.2', '回归数据', '-1');
INSERT INTO `datafile` VALUES ('17', '17', '4', 'hour_noheader.csv', '0', null, null, '2018-07-19 20:10:26.958570', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', ',', '0', '1', null, null, '0.4', '回归数据', '-1');
INSERT INTO `datafile` VALUES ('18', '18', '4', 'hour_noheader.csv', '0', null, null, '2018-07-19 20:12:44.737596', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', ',', '0', '1', null, null, '0.2', '回归数据', '-1');
INSERT INTO `datafile` VALUES ('19', '19', '4', 'hour_noheader.csv', '0', null, null, '2018-07-19 20:14:56.687741', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', ',', '1', '25', null, null, '0.3', '回归数据', '-1');
INSERT INTO `datafile` VALUES ('20', '20', '4', '丈夫属性字段.csv', '0', null, null, '2018-07-28 18:42:47.544246', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', null, null, null, null, 'c', '-1');
INSERT INTO `datafile` VALUES ('21', '21', '4', '丈夫属性字段.csv', '0', null, null, '2018-07-28 19:29:02.973925', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', null, null, null, null, 'c', '-1');
INSERT INTO `datafile` VALUES ('22', '22', '4', '丈夫属性字段.csv', '0', null, null, '2018-07-28 19:29:03.029804', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', null, null, null, null, 'c', '-1');
INSERT INTO `datafile` VALUES ('23', '23', '4', '丈夫属性字段.csv', '0', null, null, '2018-07-28 19:43:11.374432', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', null, null, null, null, 'c', '-1');
INSERT INTO `datafile` VALUES ('24', '24', '4', '丈夫属性字段.csv', '0', null, null, '2018-07-28 19:52:31.074791', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', '1', null, null, '0.2', 'c', '-1');
INSERT INTO `datafile` VALUES ('25', '25', '4', '丈夫属性字段.csv', '0', null, null, '2018-07-28 20:17:01.009132', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', '1', null, null, '0.2', 'c', '-1');
INSERT INTO `datafile` VALUES ('26', '26', '4', '丈夫属性字段.csv', '0', null, null, '2018-07-28 21:02:43.806112', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', '1', null, null, '0.2', 'c', '-1');
INSERT INTO `datafile` VALUES ('27', '27', '4', '丈夫属性字段.csv', '0', null, null, '2018-07-28 21:09:44.090671', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', '1', null, null, '0.2', 'c', '-1');
INSERT INTO `datafile` VALUES ('28', '28', '4', 'churn.csv', '0', null, null, '2018-07-28 21:31:39.507083', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', ',', '1', '1', null, null, '0.2', '分类数据', '-1');
INSERT INTO `datafile` VALUES ('29', '29', '4', 'hour_noheader.csv', '0', null, null, '2018-07-28 21:32:55.769087', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', ',', '0', null, null, null, null, '回归数据', '-1');
INSERT INTO `datafile` VALUES ('30', '31', '4', 'hour_noheader.csv', '0', null, null, '2018-07-28 21:35:49.231527', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', ',', '0', '1', null, null, '0.2', '回归数据', '-1');
INSERT INTO `datafile` VALUES ('31', null, '4', 'hour_noheader.csv', '0', null, null, '2018-07-28 21:36:06.801157', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/test1', ',', '1', null, null, null, null, 'test1', '1');
INSERT INTO `datafile` VALUES ('32', '32', '4', 'churn.csv', '0', null, null, '2018-07-28 22:03:07.989403', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', ',', '1', '1', null, null, '0.2', '分类数据', '-1');
INSERT INTO `datafile` VALUES ('33', '33', '4', 'hour_noheader.csv', '0', null, null, '2018-07-28 22:05:12.284728', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', ',', '0', '1', null, null, '0.3', '回归数据', '-1');
INSERT INTO `datafile` VALUES ('34', null, '4', 'resample_df.csv', '0', null, null, '2018-07-29 19:12:21.011509', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', ',', '1', null, null, null, null, 'PU分类', '1');
INSERT INTO `datafile` VALUES ('35', '34', '4', 'resample_df.csv', '0', null, null, '2018-07-29 19:12:31.794706', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', ',', '1', '1', null, null, '0.2', 'PU分类', '-1');
INSERT INTO `datafile` VALUES ('36', '35', '4', 'resample_df.csv', '0', null, null, '2018-07-29 21:59:22.920143', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', ',', '1', '1', null, null, '0.2', 'PU分类', '-1');
INSERT INTO `datafile` VALUES ('37', '36', '4', 'resample_df.csv', '0', null, null, '2018-07-29 22:00:48.060116', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', ',', '1', '1', null, null, '0.2', 'PU分类', '-1');
INSERT INTO `datafile` VALUES ('38', '37', '4', 'hour_noheader.csv', '0', null, null, '2018-10-06 20:32:12.786114', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/test1', ',', '1', null, null, null, null, 'test1', '-1');
INSERT INTO `datafile` VALUES ('39', null, '4', '丈夫属性字段.csv', '0', null, null, '2018-10-06 21:16:20.631626', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/关联', '，', '1', null, null, null, null, '关联', '1');
INSERT INTO `datafile` VALUES ('40', '38', '4', '丈夫属性字段.csv', '0', null, null, '2018-10-06 21:17:27.303306', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/关联', '，', '1', '1', null, null, '0.8', '关联', '-1');
INSERT INTO `datafile` VALUES ('41', '39', '4', '丈夫属性字段.csv', '0', null, null, '2018-10-06 22:24:35.441049', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/关联', '，', '1', '1', null, null, '0.8', '关联', '-1');
INSERT INTO `datafile` VALUES ('42', '42', '4', '丈夫属性字段.csv', '0', null, null, '2018-10-06 22:55:32.309201', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', '1', null, null, '0.2', 'c', '-1');
INSERT INTO `datafile` VALUES ('43', '43', '4', '丈夫属性字段.csv', '0', null, null, '2018-10-06 22:57:39.732588', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', null, null, null, null, 'c', '-1');
INSERT INTO `datafile` VALUES ('44', '44', '4', '丈夫属性字段.csv', '0', null, null, '2018-10-06 22:59:07.220465', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', null, null, null, null, 'c', '-1');
INSERT INTO `datafile` VALUES ('45', '45', '4', '丈夫属性字段.csv', '0', null, null, '2018-10-06 23:00:28.948841', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', null, null, null, null, 'c', '-1');
INSERT INTO `datafile` VALUES ('46', '46', '4', 'churn.csv', '0', null, null, '2018-10-06 23:01:49.089521', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', ',', '1', '1', null, null, '0.2', '分类数据', '-1');
INSERT INTO `datafile` VALUES ('47', '47', '4', '丈夫属性字段.csv', '0', null, null, '2018-10-06 23:18:41.222480', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/关联', '，', '1', null, null, null, null, '关联', '-1');
INSERT INTO `datafile` VALUES ('48', '48', '4', '丈夫属性字段.csv', '0', null, null, '2018-10-06 23:23:17.895622', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/关联', '，', '1', null, null, null, null, '关联', '-1');
INSERT INTO `datafile` VALUES ('49', '49', '4', '丈夫属性字段.csv', '0', null, null, '2018-10-06 23:25:14.524510', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', ',', '1', null, null, null, null, 'c', '-1');
INSERT INTO `datafile` VALUES ('50', '50', '4', 'churn.csv', '0', null, null, '2018-10-06 23:38:13.366453', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', ',', '1', '1', null, null, '0.2', '分类数据', '-1');
INSERT INTO `datafile` VALUES ('51', '51', '4', '丈夫属性字段.csv', '0', null, null, '2018-10-06 23:41:20.364022', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/关联', '，', '1', null, null, null, null, '关联', '-1');
INSERT INTO `datafile` VALUES ('52', null, '4', 'hour_noheader.csv', '0', null, null, '2018-10-07 11:53:16.832451', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/分类', '，', '1', null, null, null, null, '分类', '1');
INSERT INTO `datafile` VALUES ('53', '53', '4', 'hour_noheader.csv', '0', null, null, '2018-10-07 11:53:27.219056', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/分类', '，', '1', '1', null, null, '0.2', '分类', '-1');
INSERT INTO `datafile` VALUES ('54', null, '4', 'churn.csv', '0', null, null, '2018-10-07 12:47:33.679276', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/分类', '，', '1', null, null, null, null, '分类', '1');
INSERT INTO `datafile` VALUES ('55', '54', '4', 'churn.csv', '0', null, null, '2018-10-07 12:47:43.591692', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', ',', '1', '1', null, null, '0.2', '分类数据', '-1');
INSERT INTO `datafile` VALUES ('56', '55', '4', 'churn.csv', '0', null, null, '2018-10-07 12:52:26.852635', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', ',', '1', '1', null, null, '0.2', '分类数据', '-1');
INSERT INTO `datafile` VALUES ('57', null, '4', 'churn.csv', '0', null, null, '2018-10-07 12:57:49.433281', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/分类', '，', '1', null, null, null, null, '分类', '1');
INSERT INTO `datafile` VALUES ('58', '57', '4', 'churn.csv', '0', null, null, '2018-10-07 12:57:59.434206', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', ',', '1', '1', null, null, '0.2', '分类数据', '-1');
INSERT INTO `datafile` VALUES ('59', null, '4', 'churn.csv', '0', null, null, '2018-10-07 13:03:38.407265', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/分类', '，', '1', null, null, null, null, '分类', '1');
INSERT INTO `datafile` VALUES ('60', '58', '4', 'churn.csv', '0', null, null, '2018-10-07 13:03:46.185128', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', ',', '1', '1', null, null, '0.2', '分类数据', '-1');
INSERT INTO `datafile` VALUES ('61', null, '4', '丈夫属性字段.csv', '0', null, null, '2018-10-07 13:08:05.908853', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/关联', '，', '1', null, null, null, null, '关联', '1');
INSERT INTO `datafile` VALUES ('62', '59', '4', '丈夫属性字段.csv', '0', null, null, '2018-10-07 13:08:08.306835', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/关联', '，', '1', '1', null, null, '0.2', '关联', '-1');
INSERT INTO `datafile` VALUES ('63', null, '4', 'hour_noheader.csv', '0', null, null, '2018-10-07 13:13:06.203433', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/回归', '，', '1', null, null, null, null, '回归', '1');
INSERT INTO `datafile` VALUES ('64', '60', '4', 'hour_noheader.csv', '0', null, null, '2018-10-07 13:13:13.608394', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', ',', '0', '1', null, null, '0.2', '回归数据', '-1');
INSERT INTO `datafile` VALUES ('65', null, '4', 'resample_df.csv', '0', null, null, '2018-11-18 10:14:53.057538', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', '，', '1', null, null, null, null, 'PU分类', '1');
INSERT INTO `datafile` VALUES ('71', null, '4', 'resample_df.csv', '0', null, null, '2018-11-22 10:08:13.134869', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', '，', '1', null, null, null, null, 'PU分类', '1');
INSERT INTO `datafile` VALUES ('72', '64', '4', 'resample_df.csv', '0', null, null, '2018-11-22 10:08:14.631177', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', '，', '1', null, null, null, null, 'PU分类', '-1');
INSERT INTO `datafile` VALUES ('73', null, '4', 'resample_df.csv', '0', null, null, '2018-11-22 10:10:36.106609', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', '，', '1', null, null, null, null, 'PU分类', '1');
INSERT INTO `datafile` VALUES ('74', '65', '4', 'resample_df.csv', '0', null, null, '2018-11-22 10:10:37.163593', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', '，', '1', null, null, null, null, 'PU分类', '-1');
INSERT INTO `datafile` VALUES ('75', null, '4', 'resample_df.csv', '0', null, null, '2018-11-22 10:18:28.835753', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', '，', '1', null, null, null, null, 'PU分类', '1');
INSERT INTO `datafile` VALUES ('76', '66', '4', 'resample_df.csv', '0', null, null, '2018-11-22 10:18:29.015033', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', '，', '1', '1', null, null, '0.2', 'PU分类', '-1');

-- ----------------------------
-- Table structure for dataset
-- ----------------------------
DROP TABLE IF EXISTS `dataset`;
CREATE TABLE `dataset` (
  `userId` int(10) NOT NULL,
  `datasetName` varchar(20) NOT NULL,
  `dataType` varchar(20) NOT NULL,
  `dataPath` varchar(100) NOT NULL,
  `createTime` varchar(20) DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of dataset
-- ----------------------------
INSERT INTO `dataset` VALUES ('4', 'c', 'csv', '/home/yufeng/jsw_ml_project/data/dataset/hdp/c', '20180719183415');
INSERT INTO `dataset` VALUES ('4', '分类数据', 'csv', '/home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据', '20180719184614');
INSERT INTO `dataset` VALUES ('4', '回归数据', 'csv', '/home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据', '20180719193934');
INSERT INTO `dataset` VALUES ('4', 'test1', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/test1', '20180728213606');
INSERT INTO `dataset` VALUES ('4', 'PU分类', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', '20180729191221');
INSERT INTO `dataset` VALUES ('4', '关联', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/关联', '20181006211620');
INSERT INTO `dataset` VALUES ('4', '分类', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/分类', '20181007115316');
INSERT INTO `dataset` VALUES ('4', '分类', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/分类', '20181007124733');
INSERT INTO `dataset` VALUES ('4', '分类', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/分类', '20181007125749');
INSERT INTO `dataset` VALUES ('4', '分类', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/分类', '20181007130338');
INSERT INTO `dataset` VALUES ('4', '关联', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/关联', '20181007130807');
INSERT INTO `dataset` VALUES ('4', '回归', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/回归', '20181007131306');
INSERT INTO `dataset` VALUES ('4', 'PU分类', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', '20181118101453');
INSERT INTO `dataset` VALUES ('4', 'PU分类', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', '20181118102455');
INSERT INTO `dataset` VALUES ('4', 'PU分类', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', '20181118103049');
INSERT INTO `dataset` VALUES ('4', 'PU分类', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', '20181122100813');
INSERT INTO `dataset` VALUES ('4', 'PU分类', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', '20181122101036');
INSERT INTO `dataset` VALUES ('4', 'PU分类', 'csv', '/home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类', '20181122101828');

-- ----------------------------
-- Table structure for execution
-- ----------------------------
DROP TABLE IF EXISTS `execution`;
CREATE TABLE `execution` (
  `id` int(10) NOT NULL AUTO_INCREMENT,
  `fileid` int(50) DEFAULT NULL,
  `sessionid` int(50) DEFAULT NULL,
  `arguments` blob,
  `print_output` text,
  `outputlog` text,
  `userid` int(50) DEFAULT NULL,
  `modifiedon` datetime(6) DEFAULT NULL,
  `createdon` datetime(6) DEFAULT NULL,
  `filename` varchar(30) DEFAULT NULL,
  `filecontent` text,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=53 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of execution
-- ----------------------------
INSERT INTO `execution` VALUES ('1', '11', '1', null, 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 199ms :: artifacts dl 6ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 15:53:08 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511596388607\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1510840719610_0217/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/11/25 15:53:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 15:53:06 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 15:53:07 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 15:53:07 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 15:53:07 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 15:53:07 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 15:53:07 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 15:53:07 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 15:53:07 INFO Client: Setting up container launch context for our AM\n17/11/25 15:53:07 INFO Client: Setting up the launch environment for our AM container\n17/11/25 15:53:07 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 15:53:07 INFO Client: Preparing resources for our AM container\n17/11/25 15:53:07 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 15:53:07 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 15:53:07 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0217/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 15:53:08 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0217/org.apache.commons_commons-csv-1.1.jar\n17/11/25 15:53:08 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0217/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 15:53:08 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest3/counttest3.py\n17/11/25 15:53:08 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0217/pyspark.zip\n17/11/25 15:53:08 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0217/py4j-0.9-src.zip\n17/11/25 15:53:08 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 15:53:08 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 15:53:08 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 15:53:08 INFO Client: Uploading resource file:/tmp/spark-4d766b61-235a-4955-b062-0a23d44b8302/__spark_conf__1132431225944281141.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0217/__spark_conf__1132431225944281141.zip\n17/11/25 15:53:08 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 15:53:08 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 15:53:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 15:53:08 INFO Client: Submitting application 217 to ResourceManager\n17/11/25 15:53:08 INFO YarnClientImpl: Submitted application application_1510840719610_0217\n17/11/25 15:53:08 INFO Client: Application report for application_1510840719610_0217 (state: ACCEPTED)\n17/11/25 15:53:08 INFO Client: \n17/11/25 15:53:08 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 15:53:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-4d766b61-235a-4955-b062-0a23d44b8302\n', '4', '2017-11-25 15:54:24.164803', '2017-11-25 15:53:05.170653', null, '11');
INSERT INTO `execution` VALUES ('2', '11', '2', null, 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 201ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 15:56:24 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511596584853\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1510840719610_0218/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/11/25 15:56:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 15:56:23 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 15:56:23 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 15:56:23 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 15:56:23 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 15:56:23 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 15:56:23 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 15:56:23 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 15:56:23 INFO Client: Setting up container launch context for our AM\n17/11/25 15:56:23 INFO Client: Setting up the launch environment for our AM container\n17/11/25 15:56:24 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 15:56:24 INFO Client: Preparing resources for our AM container\n17/11/25 15:56:24 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 15:56:24 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 15:56:24 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0218/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 15:56:24 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0218/org.apache.commons_commons-csv-1.1.jar\n17/11/25 15:56:24 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0218/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 15:56:24 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest3/counttest3.py\n17/11/25 15:56:24 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0218/pyspark.zip\n17/11/25 15:56:24 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0218/py4j-0.9-src.zip\n17/11/25 15:56:24 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 15:56:24 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 15:56:24 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 15:56:24 INFO Client: Uploading resource file:/tmp/spark-01d6d6b5-085a-4f75-8b1e-c5cf5795b6af/__spark_conf__2633152074695360355.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0218/__spark_conf__2633152074695360355.zip\n17/11/25 15:56:24 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 15:56:24 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 15:56:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 15:56:24 INFO Client: Submitting application 218 to ResourceManager\n17/11/25 15:56:25 INFO YarnClientImpl: Submitted application application_1510840719610_0218\n17/11/25 15:56:25 INFO Client: Application report for application_1510840719610_0218 (state: ACCEPTED)\n17/11/25 15:56:25 INFO Client: \n17/11/25 15:56:25 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 15:56:25 INFO ShutdownHookManager: Deleting directory /tmp/spark-01d6d6b5-085a-4f75-8b1e-c5cf5795b6af\n', '4', '2017-11-25 15:57:40.236784', '2017-11-25 15:56:21.287889', null, '11');
INSERT INTO `execution` VALUES ('3', '6', '3', null, 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 204ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 16:00:12 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511596812446\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1510840719610_0219/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 16:00:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 16:00:10 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 16:00:10 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 16:00:11 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 16:00:11 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 16:00:11 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 16:00:11 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 16:00:11 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 16:00:11 INFO Client: Setting up container launch context for our AM\n17/11/25 16:00:11 INFO Client: Setting up the launch environment for our AM container\n17/11/25 16:00:11 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 16:00:11 INFO Client: Preparing resources for our AM container\n17/11/25 16:00:11 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 16:00:11 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 16:00:11 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0219/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 16:00:11 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0219/org.apache.commons_commons-csv-1.1.jar\n17/11/25 16:00:12 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0219/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 16:00:12 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/user/yufeng/hdp/code_file/pi/pi.py\n17/11/25 16:00:12 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0219/pyspark.zip\n17/11/25 16:00:12 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0219/py4j-0.9-src.zip\n17/11/25 16:00:12 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 16:00:12 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 16:00:12 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 16:00:12 INFO Client: Uploading resource file:/tmp/spark-d29f0bca-5bed-4878-a5f9-f43d45bc6cc8/__spark_conf__674140041992365002.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1510840719610_0219/__spark_conf__674140041992365002.zip\n17/11/25 16:00:12 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 16:00:12 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 16:00:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 16:00:12 INFO Client: Submitting application 219 to ResourceManager\n17/11/25 16:00:12 INFO YarnClientImpl: Submitted application application_1510840719610_0219\n17/11/25 16:00:12 INFO Client: Application report for application_1510840719610_0219 (state: ACCEPTED)\n17/11/25 16:00:12 INFO Client: \n17/11/25 16:00:12 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 16:00:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-d29f0bca-5bed-4878-a5f9-f43d45bc6cc8\n', '4', '2017-11-25 16:00:57.801505', '2017-11-25 16:00:08.885017', null, '6');
INSERT INTO `execution` VALUES ('4', '6', '4', null, 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 216ms :: artifacts dl 6ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 16:22:26 +0800 2017] Scheduler has assigned a container for AM, waiting for AM container to be launched\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511598146281\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598013543_0001/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 16:22:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 16:22:24 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 16:22:24 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 16:22:24 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 16:22:25 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 16:22:25 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 16:22:25 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 16:22:25 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 16:22:25 INFO Client: Setting up container launch context for our AM\n17/11/25 16:22:25 INFO Client: Setting up the launch environment for our AM container\n17/11/25 16:22:25 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 16:22:25 INFO Client: Preparing resources for our AM container\n17/11/25 16:22:25 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 16:22:25 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 16:22:25 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598013543_0001/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 16:22:25 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598013543_0001/org.apache.commons_commons-csv-1.1.jar\n17/11/25 16:22:25 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598013543_0001/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 16:22:25 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/user/yufeng/hdp/code_file/pi/pi.py\n17/11/25 16:22:25 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598013543_0001/pyspark.zip\n17/11/25 16:22:25 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598013543_0001/py4j-0.9-src.zip\n17/11/25 16:22:26 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 16:22:26 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 16:22:26 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 16:22:26 INFO Client: Uploading resource file:/tmp/spark-70d2eaf0-1744-4e0c-89aa-ae348179f261/__spark_conf__5234331695093812616.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598013543_0001/__spark_conf__5234331695093812616.zip\n17/11/25 16:22:26 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 16:22:26 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 16:22:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 16:22:26 INFO Client: Submitting application 1 to ResourceManager\n17/11/25 16:22:26 INFO YarnClientImpl: Submitted application application_1511598013543_0001\n17/11/25 16:22:26 INFO Client: Application report for application_1511598013543_0001 (state: ACCEPTED)\n17/11/25 16:22:26 INFO Client: \n17/11/25 16:22:26 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 16:22:26 INFO ShutdownHookManager: Deleting directory /tmp/spark-70d2eaf0-1744-4e0c-89aa-ae348179f261\n', '4', '2017-11-25 16:23:11.725263', '2017-11-25 16:22:22.668114', null, '6');
INSERT INTO `execution` VALUES ('5', '6', '5', null, 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 202ms :: artifacts dl 4ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 16:52:28 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511599948320\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0001/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 16:52:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 16:52:26 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 16:52:26 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 16:52:26 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 16:52:27 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 16:52:27 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 16:52:27 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 16:52:27 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 16:52:27 INFO Client: Setting up container launch context for our AM\n17/11/25 16:52:27 INFO Client: Setting up the launch environment for our AM container\n17/11/25 16:52:27 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 16:52:27 INFO Client: Preparing resources for our AM container\n17/11/25 16:52:27 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 16:52:27 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 16:52:27 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0001/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 16:52:27 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0001/org.apache.commons_commons-csv-1.1.jar\n17/11/25 16:52:27 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0001/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 16:52:27 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/user/yufeng/hdp/code_file/pi/pi.py\n17/11/25 16:52:27 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0001/pyspark.zip\n17/11/25 16:52:27 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0001/py4j-0.9-src.zip\n17/11/25 16:52:28 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 16:52:28 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 16:52:28 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 16:52:28 INFO Client: Uploading resource file:/tmp/spark-1a64f3a2-e250-4cdd-afa6-4d1bc43004fa/__spark_conf__4547622402620612539.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0001/__spark_conf__4547622402620612539.zip\n17/11/25 16:52:28 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 16:52:28 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 16:52:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 16:52:28 INFO Client: Submitting application 1 to ResourceManager\n17/11/25 16:52:28 INFO YarnClientImpl: Submitted application application_1511598448747_0001\n17/11/25 16:52:28 INFO Client: Application report for application_1511598448747_0001 (state: ACCEPTED)\n17/11/25 16:52:28 INFO Client: \n17/11/25 16:52:28 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 16:52:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-1a64f3a2-e250-4cdd-afa6-4d1bc43004fa\n', '4', '2017-11-25 16:53:13.759779', '2017-11-25 16:52:24.990611', null, '6');
INSERT INTO `execution` VALUES ('6', '12', '6', null, 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 199ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 17:20:20 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511601620095\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0002/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 17:20:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 17:20:18 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 17:20:18 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 17:20:18 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 17:20:19 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 17:20:19 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 17:20:19 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 17:20:19 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 17:20:19 INFO Client: Setting up container launch context for our AM\n17/11/25 17:20:19 INFO Client: Setting up the launch environment for our AM container\n17/11/25 17:20:19 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 17:20:19 INFO Client: Preparing resources for our AM container\n17/11/25 17:20:19 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 17:20:19 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 17:20:19 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0002/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 17:20:19 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0002/org.apache.commons_commons-csv-1.1.jar\n17/11/25 17:20:19 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0002/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 17:20:19 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi/save_pi.py\n17/11/25 17:20:19 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0002/pyspark.zip\n17/11/25 17:20:19 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0002/py4j-0.9-src.zip\n17/11/25 17:20:19 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 17:20:19 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 17:20:19 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 17:20:19 INFO Client: Uploading resource file:/tmp/spark-3ad16a33-5b6a-488f-8975-248417e18a54/__spark_conf__5729585794471633875.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0002/__spark_conf__5729585794471633875.zip\n17/11/25 17:20:19 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 17:20:19 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 17:20:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 17:20:20 INFO Client: Submitting application 2 to ResourceManager\n17/11/25 17:20:20 INFO YarnClientImpl: Submitted application application_1511598448747_0002\n17/11/25 17:20:20 INFO Client: Application report for application_1511598448747_0002 (state: ACCEPTED)\n17/11/25 17:20:20 INFO Client: \n17/11/25 17:20:20 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 17:20:20 INFO ShutdownHookManager: Deleting directory /tmp/spark-3ad16a33-5b6a-488f-8975-248417e18a54\n', '4', '2017-11-25 17:21:00.455947', '2017-11-25 17:20:16.685876', null, '12');
INSERT INTO `execution` VALUES ('7', '14', '7', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 206ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 17:34:07 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511602447087\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0003/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/11/25 17:34:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 17:34:05 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 17:34:05 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 17:34:05 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 17:34:05 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 17:34:06 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 17:34:06 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 17:34:06 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 17:34:06 INFO Client: Setting up container launch context for our AM\n17/11/25 17:34:06 INFO Client: Setting up the launch environment for our AM container\n17/11/25 17:34:06 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 17:34:06 INFO Client: Preparing resources for our AM container\n17/11/25 17:34:06 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 17:34:06 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 17:34:06 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0003/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 17:34:06 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0003/org.apache.commons_commons-csv-1.1.jar\n17/11/25 17:34:06 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0003/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 17:34:06 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi/save_pi.py\n17/11/25 17:34:06 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0003/pyspark.zip\n17/11/25 17:34:06 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0003/py4j-0.9-src.zip\n17/11/25 17:34:06 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 17:34:06 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 17:34:06 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 17:34:06 INFO Client: Uploading resource file:/tmp/spark-648c4f1b-cd24-4021-9700-257986cff349/__spark_conf__2302177375412965463.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0003/__spark_conf__2302177375412965463.zip\n17/11/25 17:34:06 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 17:34:06 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 17:34:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 17:34:07 INFO Client: Submitting application 3 to ResourceManager\n17/11/25 17:34:07 INFO YarnClientImpl: Submitted application application_1511598448747_0003\n17/11/25 17:34:07 INFO Client: Application report for application_1511598448747_0003 (state: ACCEPTED)\n17/11/25 17:34:07 INFO Client: \n17/11/25 17:34:07 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 17:34:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-648c4f1b-cd24-4021-9700-257986cff349\n', '4', '2017-11-25 17:35:27.485247', '2017-11-25 17:34:03.582389', null, 'import time\nfrom pyspark import SparkContext\nimport random\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = (4.0 * count / NUM_SAMPLES)\n    sc.parallelize(str_pi).repartition(1).saveAsTextFile(\"/user/yufeng/text.txt\")\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('8', '15', '8', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 183ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/5ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511602823513\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0004/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 17:40:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 17:40:21 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 17:40:21 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 17:40:22 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 17:40:22 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 17:40:22 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 17:40:22 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 17:40:22 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 17:40:22 INFO Client: Setting up container launch context for our AM\n17/11/25 17:40:22 INFO Client: Setting up the launch environment for our AM container\n17/11/25 17:40:22 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 17:40:22 INFO Client: Preparing resources for our AM container\n17/11/25 17:40:22 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 17:40:22 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 17:40:22 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0004/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 17:40:23 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0004/org.apache.commons_commons-csv-1.1.jar\n17/11/25 17:40:23 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0004/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 17:40:23 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi_v2/save_pi_v2.py\n17/11/25 17:40:23 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0004/pyspark.zip\n17/11/25 17:40:23 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0004/py4j-0.9-src.zip\n17/11/25 17:40:23 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 17:40:23 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 17:40:23 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 17:40:23 INFO Client: Uploading resource file:/tmp/spark-313c4336-0969-4049-b9ac-02ff054050e9/__spark_conf__7454807106065774330.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0004/__spark_conf__7454807106065774330.zip\n17/11/25 17:40:23 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 17:40:23 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 17:40:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 17:40:23 INFO Client: Submitting application 4 to ResourceManager\n17/11/25 17:40:23 INFO YarnClientImpl: Submitted application application_1511598448747_0004\n17/11/25 17:40:23 INFO Client: Application report for application_1511598448747_0004 (state: ACCEPTED)\n17/11/25 17:40:23 INFO Client: \n17/11/25 17:40:23 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 17:40:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-313c4336-0969-4049-b9ac-02ff054050e9\n', '4', '2017-11-25 17:41:08.870720', '2017-11-25 17:40:20.148561', null, 'import time\nfrom pyspark import SparkContext\nimport random\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    List = [str_pi]\n    sc.parallelize(List).repartition(1).saveAsTextFile(\"/user/yufeng/text\")\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('9', '15', '9', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 253ms :: artifacts dl 4ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 17:43:30 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511603010076\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0005/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/11/25 17:43:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 17:43:28 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 17:43:28 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 17:43:28 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 17:43:28 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 17:43:29 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 17:43:29 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 17:43:29 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 17:43:29 INFO Client: Setting up container launch context for our AM\n17/11/25 17:43:29 INFO Client: Setting up the launch environment for our AM container\n17/11/25 17:43:29 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 17:43:29 INFO Client: Preparing resources for our AM container\n17/11/25 17:43:29 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 17:43:29 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 17:43:29 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0005/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 17:43:29 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0005/org.apache.commons_commons-csv-1.1.jar\n17/11/25 17:43:29 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0005/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 17:43:29 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi_v2/save_pi_v2.py\n17/11/25 17:43:29 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0005/pyspark.zip\n17/11/25 17:43:29 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0005/py4j-0.9-src.zip\n17/11/25 17:43:29 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 17:43:29 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 17:43:29 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 17:43:29 INFO Client: Uploading resource file:/tmp/spark-05478863-6505-405f-858f-069909abf414/__spark_conf__6690257982259078130.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0005/__spark_conf__6690257982259078130.zip\n17/11/25 17:43:29 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 17:43:29 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 17:43:29 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 17:43:30 INFO Client: Submitting application 5 to ResourceManager\n17/11/25 17:43:30 INFO YarnClientImpl: Submitted application application_1511598448747_0005\n17/11/25 17:43:30 INFO Client: Application report for application_1511598448747_0005 (state: ACCEPTED)\n17/11/25 17:43:30 INFO Client: \n17/11/25 17:43:30 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 17:43:30 INFO ShutdownHookManager: Deleting directory /tmp/spark-05478863-6505-405f-858f-069909abf414\n', '4', '2017-11-25 17:44:50.439716', '2017-11-25 17:43:26.497079', null, 'import time\nfrom pyspark import SparkContext\nimport random\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    List = [str_pi]\n    sc.parallelize(List).repartition(1).saveAsTextFile(\"/user/yufeng/text\")\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('10', '6', '10', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 187ms :: artifacts dl 4ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/5ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 18:10:18 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511604618142\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0006/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 18:10:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 18:10:16 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 18:10:16 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 18:10:16 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 18:10:17 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 18:10:17 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 18:10:17 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 18:10:17 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 18:10:17 INFO Client: Setting up container launch context for our AM\n17/11/25 18:10:17 INFO Client: Setting up the launch environment for our AM container\n17/11/25 18:10:17 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 18:10:17 INFO Client: Preparing resources for our AM container\n17/11/25 18:10:17 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 18:10:17 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 18:10:17 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0006/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 18:10:17 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0006/org.apache.commons_commons-csv-1.1.jar\n17/11/25 18:10:17 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0006/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 18:10:17 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/user/yufeng/hdp/code_file/pi/pi.py\n17/11/25 18:10:17 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0006/pyspark.zip\n17/11/25 18:10:17 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0006/py4j-0.9-src.zip\n17/11/25 18:10:17 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 18:10:17 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 18:10:17 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 18:10:17 INFO Client: Uploading resource file:/tmp/spark-c7a12f94-9ec8-4b18-872b-05fc795ccfd1/__spark_conf__2609140472050947619.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0006/__spark_conf__2609140472050947619.zip\n17/11/25 18:10:17 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 18:10:17 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 18:10:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 18:10:18 INFO Client: Submitting application 6 to ResourceManager\n17/11/25 18:10:18 INFO YarnClientImpl: Submitted application application_1511598448747_0006\n17/11/25 18:10:18 INFO Client: Application report for application_1511598448747_0006 (state: ACCEPTED)\n17/11/25 18:10:18 INFO Client: \n17/11/25 18:10:18 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 18:10:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-c7a12f94-9ec8-4b18-872b-05fc795ccfd1\n', '4', '2017-11-25 18:10:58.511062', '2017-11-25 18:10:14.675961', null, 'import time\nfrom pyspark import SparkContext\nimport random\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('11', '16', '11', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 199ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 19:28:07 +0800 2017] Scheduler has assigned a container for AM, waiting for AM container to be launched\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511609287344\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0007/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 19:28:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 19:28:05 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 19:28:05 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 19:28:05 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 19:28:06 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 19:28:06 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 19:28:06 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 19:28:06 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 19:28:06 INFO Client: Setting up container launch context for our AM\n17/11/25 19:28:06 INFO Client: Setting up the launch environment for our AM container\n17/11/25 19:28:06 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 19:28:06 INFO Client: Preparing resources for our AM container\n17/11/25 19:28:06 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 19:28:06 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 19:28:06 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0007/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 19:28:06 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0007/org.apache.commons_commons-csv-1.1.jar\n17/11/25 19:28:06 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0007/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 19:28:06 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi_v3/save_pi_v3.py\n17/11/25 19:28:06 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0007/pyspark.zip\n17/11/25 19:28:07 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0007/py4j-0.9-src.zip\n17/11/25 19:28:07 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 19:28:07 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 19:28:07 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 19:28:07 INFO Client: Uploading resource file:/tmp/spark-2061d472-b87d-4c4c-b91f-61b9bd95d4c8/__spark_conf__7179102536180359132.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0007/__spark_conf__7179102536180359132.zip\n17/11/25 19:28:07 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 19:28:07 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 19:28:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 19:28:07 INFO Client: Submitting application 7 to ResourceManager\n17/11/25 19:28:07 INFO YarnClientImpl: Submitted application application_1511598448747_0007\n17/11/25 19:28:07 INFO Client: Application report for application_1511598448747_0007 (state: ACCEPTED)\n17/11/25 19:28:07 INFO Client: \n17/11/25 19:28:07 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 19:28:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-2061d472-b87d-4c4c-b91f-61b9bd95d4c8\n', '4', '2017-11-25 19:28:47.696743', '2017-11-25 19:28:03.959333', null, 'import time\nfrom pyspark import SparkContext\nimport random\nimport sys\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    List = [str_pi]\n    sc.parallelize(List).repartition(1).saveAsTextFile(sys.argv[1])\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('12', '16', '12', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 231ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/9ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 19:58:46 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511611126872\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0008/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 19:58:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 19:58:45 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 19:58:45 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 19:58:45 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 19:58:45 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 19:58:45 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 19:58:45 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 19:58:45 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 19:58:45 INFO Client: Setting up container launch context for our AM\n17/11/25 19:58:45 INFO Client: Setting up the launch environment for our AM container\n17/11/25 19:58:45 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 19:58:45 INFO Client: Preparing resources for our AM container\n17/11/25 19:58:46 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 19:58:46 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 19:58:46 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0008/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 19:58:46 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0008/org.apache.commons_commons-csv-1.1.jar\n17/11/25 19:58:46 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0008/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 19:58:46 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi_v3/save_pi_v3.py\n17/11/25 19:58:46 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0008/pyspark.zip\n17/11/25 19:58:46 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0008/py4j-0.9-src.zip\n17/11/25 19:58:46 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 19:58:46 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 19:58:46 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 19:58:46 INFO Client: Uploading resource file:/tmp/spark-93ffbe0e-e5c8-423d-bf47-94a7788e18f2/__spark_conf__4848322761274833342.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0008/__spark_conf__4848322761274833342.zip\n17/11/25 19:58:46 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 19:58:46 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 19:58:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 19:58:46 INFO Client: Submitting application 8 to ResourceManager\n17/11/25 19:58:47 INFO YarnClientImpl: Submitted application application_1511598448747_0008\n17/11/25 19:58:47 INFO Client: Application report for application_1511598448747_0008 (state: ACCEPTED)\n17/11/25 19:58:47 INFO Client: \n17/11/25 19:58:47 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 19:58:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-93ffbe0e-e5c8-423d-bf47-94a7788e18f2\n', '4', '2017-11-25 19:59:27.225730', '2017-11-25 19:58:41.382126', null, 'import time\nfrom pyspark import SparkContext\nimport random\nimport sys\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    List = [str_pi]\n    sc.parallelize(List).repartition(1).saveAsTextFile(sys.argv[1])\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('13', '16', '13', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 221ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/7ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 20:02:49 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511611369440\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0009/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 20:02:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 20:02:47 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 20:02:47 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 20:02:47 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 20:02:48 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 20:02:48 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 20:02:48 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 20:02:48 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 20:02:48 INFO Client: Setting up container launch context for our AM\n17/11/25 20:02:48 INFO Client: Setting up the launch environment for our AM container\n17/11/25 20:02:48 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:02:48 INFO Client: Preparing resources for our AM container\n17/11/25 20:02:48 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:02:48 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:02:48 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0009/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 20:02:48 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0009/org.apache.commons_commons-csv-1.1.jar\n17/11/25 20:02:48 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0009/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 20:02:49 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi_v3/save_pi_v3.py\n17/11/25 20:02:49 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0009/pyspark.zip\n17/11/25 20:02:49 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0009/py4j-0.9-src.zip\n17/11/25 20:02:49 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 20:02:49 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 20:02:49 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 20:02:49 INFO Client: Uploading resource file:/tmp/spark-b064af0e-7596-4a7f-a728-8dbf437cb76e/__spark_conf__9026410949357926431.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0009/__spark_conf__9026410949357926431.zip\n17/11/25 20:02:49 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 20:02:49 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 20:02:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 20:02:49 INFO Client: Submitting application 9 to ResourceManager\n17/11/25 20:02:49 INFO YarnClientImpl: Submitted application application_1511598448747_0009\n17/11/25 20:02:49 INFO Client: Application report for application_1511598448747_0009 (state: ACCEPTED)\n17/11/25 20:02:49 INFO Client: \n17/11/25 20:02:49 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 20:02:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-b064af0e-7596-4a7f-a728-8dbf437cb76e\n', '4', '2017-11-25 20:03:03.583801', '2017-11-25 20:02:43.769442', null, 'import time\nfrom pyspark import SparkContext\nimport random\nimport sys\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    List = [str_pi]\n    sc.parallelize(List).repartition(1).saveAsTextFile(sys.argv[1])\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('14', '16', '14', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 210ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/7ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 20:02:51 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 20.454546 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511611371309\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0010/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 20:02:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 20:02:49 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 20:02:49 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 20:02:49 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 20:02:50 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 20:02:50 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 20:02:50 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 20:02:50 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 20:02:50 INFO Client: Setting up container launch context for our AM\n17/11/25 20:02:50 INFO Client: Setting up the launch environment for our AM container\n17/11/25 20:02:50 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:02:50 INFO Client: Preparing resources for our AM container\n17/11/25 20:02:50 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:02:50 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:02:50 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0010/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 20:02:50 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0010/org.apache.commons_commons-csv-1.1.jar\n17/11/25 20:02:50 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0010/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 20:02:50 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi_v3/save_pi_v3.py\n17/11/25 20:02:50 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0010/pyspark.zip\n17/11/25 20:02:50 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0010/py4j-0.9-src.zip\n17/11/25 20:02:51 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 20:02:51 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 20:02:51 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 20:02:51 INFO Client: Uploading resource file:/tmp/spark-89fa2430-f569-417b-a387-e60b895f7e98/__spark_conf__7756480599527773234.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0010/__spark_conf__7756480599527773234.zip\n17/11/25 20:02:51 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 20:02:51 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 20:02:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 20:02:51 INFO Client: Submitting application 10 to ResourceManager\n17/11/25 20:02:51 INFO YarnClientImpl: Submitted application application_1511598448747_0010\n17/11/25 20:02:51 INFO Client: Application report for application_1511598448747_0010 (state: ACCEPTED)\n17/11/25 20:02:51 INFO Client: \n17/11/25 20:02:51 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 20:02:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-89fa2430-f569-417b-a387-e60b895f7e98\n', '4', '2017-11-25 20:03:05.144421', '2017-11-25 20:02:45.790681', null, 'import time\nfrom pyspark import SparkContext\nimport random\nimport sys\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    List = [str_pi]\n    sc.parallelize(List).repartition(1).saveAsTextFile(sys.argv[1])\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('15', '16', '15', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 195ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/5ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511611495815\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0011/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/11/25 20:04:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 20:04:54 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 20:04:54 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 20:04:54 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 20:04:54 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 20:04:54 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 20:04:54 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 20:04:54 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 20:04:54 INFO Client: Setting up container launch context for our AM\n17/11/25 20:04:54 INFO Client: Setting up the launch environment for our AM container\n17/11/25 20:04:54 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:04:54 INFO Client: Preparing resources for our AM container\n17/11/25 20:04:54 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:04:54 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:04:55 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0011/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 20:04:55 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0011/org.apache.commons_commons-csv-1.1.jar\n17/11/25 20:04:55 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0011/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 20:04:55 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi_v3/save_pi_v3.py\n17/11/25 20:04:55 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0011/pyspark.zip\n17/11/25 20:04:55 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0011/py4j-0.9-src.zip\n17/11/25 20:04:55 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 20:04:55 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 20:04:55 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 20:04:55 INFO Client: Uploading resource file:/tmp/spark-5029589d-ab3a-4dd2-876f-5c1cba726d84/__spark_conf__1665242755138142385.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0011/__spark_conf__1665242755138142385.zip\n17/11/25 20:04:55 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 20:04:55 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 20:04:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 20:04:55 INFO Client: Submitting application 11 to ResourceManager\n17/11/25 20:04:56 INFO YarnClientImpl: Submitted application application_1511598448747_0011\n17/11/25 20:04:56 INFO Client: Application report for application_1511598448747_0011 (state: ACCEPTED)\n17/11/25 20:04:56 INFO Client: \n17/11/25 20:04:56 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 20:04:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-5029589d-ab3a-4dd2-876f-5c1cba726d84\n', '4', '2017-11-25 20:06:16.158943', '2017-11-25 20:04:50.307114', null, 'import time\nfrom pyspark import SparkContext\nimport random\nimport sys\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    List = [str_pi]\n    sc.parallelize(List).repartition(1).saveAsTextFile(sys.argv[1])\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('16', '16', '16', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 204ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 20:07:31 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511611651305\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0012/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 20:07:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 20:07:29 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 20:07:29 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 20:07:29 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 20:07:30 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 20:07:30 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 20:07:30 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 20:07:30 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 20:07:30 INFO Client: Setting up container launch context for our AM\n17/11/25 20:07:30 INFO Client: Setting up the launch environment for our AM container\n17/11/25 20:07:30 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:07:30 INFO Client: Preparing resources for our AM container\n17/11/25 20:07:30 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:07:30 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:07:30 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0012/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 20:07:30 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0012/org.apache.commons_commons-csv-1.1.jar\n17/11/25 20:07:30 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0012/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 20:07:30 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi_v3/save_pi_v3.py\n17/11/25 20:07:30 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0012/pyspark.zip\n17/11/25 20:07:30 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0012/py4j-0.9-src.zip\n17/11/25 20:07:31 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 20:07:31 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 20:07:31 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 20:07:31 INFO Client: Uploading resource file:/tmp/spark-257070eb-ef6a-469a-b02e-f68e17775719/__spark_conf__4932922857020909999.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0012/__spark_conf__4932922857020909999.zip\n17/11/25 20:07:31 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 20:07:31 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 20:07:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 20:07:31 INFO Client: Submitting application 12 to ResourceManager\n17/11/25 20:07:31 INFO YarnClientImpl: Submitted application application_1511598448747_0012\n17/11/25 20:07:31 INFO Client: Application report for application_1511598448747_0012 (state: ACCEPTED)\n17/11/25 20:07:31 INFO Client: \n17/11/25 20:07:31 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 20:07:31 INFO ShutdownHookManager: Deleting directory /tmp/spark-257070eb-ef6a-469a-b02e-f68e17775719\n', '4', '2017-11-25 20:08:11.629567', '2017-11-25 20:07:25.768640', null, 'import time\nfrom pyspark import SparkContext\nimport random\nimport sys\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    List = [str_pi]\n    sc.parallelize(List).repartition(1).saveAsTextFile(sys.argv[1])\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('17', '16', '17', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 196ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/5ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511611887744\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0013/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 20:11:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 20:11:26 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 20:11:26 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 20:11:26 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 20:11:26 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 20:11:26 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 20:11:26 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 20:11:26 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 20:11:26 INFO Client: Setting up container launch context for our AM\n17/11/25 20:11:26 INFO Client: Setting up the launch environment for our AM container\n17/11/25 20:11:26 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:11:26 INFO Client: Preparing resources for our AM container\n17/11/25 20:11:26 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:11:26 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:11:27 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0013/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 20:11:27 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0013/org.apache.commons_commons-csv-1.1.jar\n17/11/25 20:11:27 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0013/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 20:11:27 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi_v3/save_pi_v3.py\n17/11/25 20:11:27 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0013/pyspark.zip\n17/11/25 20:11:27 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0013/py4j-0.9-src.zip\n17/11/25 20:11:27 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 20:11:27 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 20:11:27 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 20:11:27 INFO Client: Uploading resource file:/tmp/spark-59daea38-aa02-40b9-9678-fcb7baa0acbb/__spark_conf__3703075141611423341.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0013/__spark_conf__3703075141611423341.zip\n17/11/25 20:11:27 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 20:11:27 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 20:11:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 20:11:27 INFO Client: Submitting application 13 to ResourceManager\n17/11/25 20:11:27 INFO YarnClientImpl: Submitted application application_1511598448747_0013\n17/11/25 20:11:27 INFO Client: Application report for application_1511598448747_0013 (state: ACCEPTED)\n17/11/25 20:11:27 INFO Client: \n17/11/25 20:11:27 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 20:11:27 INFO ShutdownHookManager: Deleting directory /tmp/spark-59daea38-aa02-40b9-9678-fcb7baa0acbb\n', '4', '2017-11-25 20:12:13.095729', '2017-11-25 20:11:22.526144', null, 'import time\nfrom pyspark import SparkContext\nimport random\nimport sys\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    List = [str_pi]\n    sc.parallelize(List).repartition(1).saveAsTextFile(sys.argv[1])\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('18', '16', '18', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 208ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 20:14:36 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511612076421\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0014/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 20:14:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 20:14:34 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 20:14:34 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 20:14:35 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 20:14:35 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 20:14:35 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 20:14:35 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 20:14:35 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 20:14:35 INFO Client: Setting up container launch context for our AM\n17/11/25 20:14:35 INFO Client: Setting up the launch environment for our AM container\n17/11/25 20:14:35 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:14:35 INFO Client: Preparing resources for our AM container\n17/11/25 20:14:35 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:14:35 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:14:35 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0014/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 20:14:35 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0014/org.apache.commons_commons-csv-1.1.jar\n17/11/25 20:14:35 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0014/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 20:14:36 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi_v3/save_pi_v3.py\n17/11/25 20:14:36 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0014/pyspark.zip\n17/11/25 20:14:36 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0014/py4j-0.9-src.zip\n17/11/25 20:14:36 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 20:14:36 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 20:14:36 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 20:14:36 INFO Client: Uploading resource file:/tmp/spark-d54ebb5d-da29-4555-a4fe-bd7d9b98704a/__spark_conf__8194854159197142904.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0014/__spark_conf__8194854159197142904.zip\n17/11/25 20:14:36 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 20:14:36 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 20:14:36 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 20:14:36 INFO Client: Submitting application 14 to ResourceManager\n17/11/25 20:14:36 INFO YarnClientImpl: Submitted application application_1511598448747_0014\n17/11/25 20:14:36 INFO Client: Application report for application_1511598448747_0014 (state: ACCEPTED)\n17/11/25 20:14:36 INFO Client: \n17/11/25 20:14:36 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 20:14:36 INFO ShutdownHookManager: Deleting directory /tmp/spark-d54ebb5d-da29-4555-a4fe-bd7d9b98704a\n', '4', '2017-11-25 20:15:21.782634', '2017-11-25 20:14:31.028480', null, 'import time\nfrom pyspark import SparkContext\nimport random\nimport sys\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    List = [str_pi]\n    sc.parallelize(List).repartition(1).saveAsTextFile(sys.argv[1])\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('19', '16', '19', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 204ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/5ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511612394530\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0015/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 20:19:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 20:19:52 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 20:19:53 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 20:19:53 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 20:19:53 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 20:19:53 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 20:19:53 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 20:19:53 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 20:19:53 INFO Client: Setting up container launch context for our AM\n17/11/25 20:19:53 INFO Client: Setting up the launch environment for our AM container\n17/11/25 20:19:53 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:19:53 INFO Client: Preparing resources for our AM container\n17/11/25 20:19:53 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:19:53 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:19:53 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0015/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 20:19:54 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0015/org.apache.commons_commons-csv-1.1.jar\n17/11/25 20:19:54 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0015/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 20:19:54 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi_v3/save_pi_v3.py\n17/11/25 20:19:54 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0015/pyspark.zip\n17/11/25 20:19:54 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0015/py4j-0.9-src.zip\n17/11/25 20:19:54 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 20:19:54 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 20:19:54 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 20:19:54 INFO Client: Uploading resource file:/tmp/spark-610afb77-b50c-4238-84a9-fc573509e35c/__spark_conf__2541112067768494771.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0015/__spark_conf__2541112067768494771.zip\n17/11/25 20:19:54 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 20:19:54 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 20:19:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 20:19:54 INFO Client: Submitting application 15 to ResourceManager\n17/11/25 20:19:54 INFO YarnClientImpl: Submitted application application_1511598448747_0015\n17/11/25 20:19:54 INFO Client: Application report for application_1511598448747_0015 (state: ACCEPTED)\n17/11/25 20:19:54 INFO Client: \n17/11/25 20:19:54 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 20:19:54 INFO ShutdownHookManager: Deleting directory /tmp/spark-610afb77-b50c-4238-84a9-fc573509e35c\n', '4', '2017-11-25 20:20:34.847144', '2017-11-25 20:19:49.192747', null, 'import time\nfrom pyspark import SparkContext\nimport random\nimport sys\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    List = [str_pi]\n    sc.parallelize(List).repartition(1).saveAsTextFile(sys.argv[1])\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('20', '8', '20', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 205ms :: artifacts dl 6ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 20:24:48 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511612688659\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0016/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nApplication application_1511598448747_0016 failed 2 times due to AM Container for appattempt_1511598448747_0016_000002 exited with  exitCode: 1\nFor more detailed output, check the application tracking page: http://master.hadoop:8088/cluster/app/application_1511598448747_0016 Then click on links to logs of each attempt.\nDiagnostics: Exception from container-launch.\nContainer id: container_e42_1511598448747_0016_02_000001\nExit code: 1\nStack trace: ExitCodeException exitCode=1: \n	at org.apache.hadoop.util.Shell.runCommand(Shell.java:933)\n	at org.apache.hadoop.util.Shell.run(Shell.java:844)\n	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1123)\n	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:225)\n	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:317)\n	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:83)\n	at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n	at java.lang.Thread.run(Thread.java:745)\n\n\nContainer exited with a non-zero exit code 1\nFailing this attempt. Failing the application.\n', '17/11/25 20:24:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 20:24:47 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 20:24:47 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 20:24:47 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 20:24:47 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 20:24:47 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 20:24:47 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 20:24:47 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 20:24:47 INFO Client: Setting up container launch context for our AM\n17/11/25 20:24:47 INFO Client: Setting up the launch environment for our AM container\n17/11/25 20:24:47 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:24:47 INFO Client: Preparing resources for our AM container\n17/11/25 20:24:47 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:24:47 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:24:48 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0016/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 20:24:48 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0016/org.apache.commons_commons-csv-1.1.jar\n17/11/25 20:24:48 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0016/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 20:24:48 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest/counttest.py\n17/11/25 20:24:48 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0016/pyspark.zip\n17/11/25 20:24:48 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0016/py4j-0.9-src.zip\n17/11/25 20:24:48 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 20:24:48 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 20:24:48 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 20:24:48 INFO Client: Uploading resource file:/tmp/spark-01c6d2de-d6f5-41a9-9365-c96c8097659f/__spark_conf__668277668588334843.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0016/__spark_conf__668277668588334843.zip\n17/11/25 20:24:48 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 20:24:48 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 20:24:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 20:24:48 INFO Client: Submitting application 16 to ResourceManager\n17/11/25 20:24:48 INFO YarnClientImpl: Submitted application application_1511598448747_0016\n17/11/25 20:24:48 INFO Client: Application report for application_1511598448747_0016 (state: ACCEPTED)\n17/11/25 20:24:48 INFO Client: \n17/11/25 20:24:48 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 20:24:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-01c6d2de-d6f5-41a9-9365-c96c8097659f\n', '4', '2017-11-25 20:25:18.988209', '2017-11-25 20:24:43.094979', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom operator import add\n\nif __name__ == \"__main__\":\n	if len(sys.argv) != 2:\n		print(\"Usage: counttest <file>\", file=sys.stderr)\n		exit(-1)\n\n	sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('21', '17', '21', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 227ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 20:27:04 +0800 2017] Scheduler has assigned a container for AM, waiting for AM container to be launched\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511612824354\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0017/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/11/25 20:27:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 20:27:02 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 20:27:02 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 20:27:02 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 20:27:03 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 20:27:03 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 20:27:03 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 20:27:03 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 20:27:03 INFO Client: Setting up container launch context for our AM\n17/11/25 20:27:03 INFO Client: Setting up the launch environment for our AM container\n17/11/25 20:27:03 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:27:03 INFO Client: Preparing resources for our AM container\n17/11/25 20:27:03 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:27:03 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:27:03 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0017/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 20:27:03 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0017/org.apache.commons_commons-csv-1.1.jar\n17/11/25 20:27:03 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0017/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 20:27:03 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest4/counttest4.py\n17/11/25 20:27:03 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0017/pyspark.zip\n17/11/25 20:27:04 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0017/py4j-0.9-src.zip\n17/11/25 20:27:04 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 20:27:04 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 20:27:04 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 20:27:04 INFO Client: Uploading resource file:/tmp/spark-79b249b8-bd25-49f7-b26b-9a03571c0e93/__spark_conf__4108530884634879336.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0017/__spark_conf__4108530884634879336.zip\n17/11/25 20:27:04 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 20:27:04 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 20:27:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 20:27:04 INFO Client: Submitting application 17 to ResourceManager\n17/11/25 20:27:04 INFO YarnClientImpl: Submitted application application_1511598448747_0017\n17/11/25 20:27:04 INFO Client: Application report for application_1511598448747_0017 (state: ACCEPTED)\n17/11/25 20:27:04 INFO Client: \n17/11/25 20:27:04 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 20:27:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-79b249b8-bd25-49f7-b26b-9a03571c0e93\n', '4', '2017-11-25 20:28:19.731719', '2017-11-25 20:26:58.950827', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('22', '17', '22', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 193ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/5ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511613536770\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0018/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/11/25 20:38:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 20:38:55 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 20:38:55 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 20:38:55 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 20:38:55 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 20:38:55 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 20:38:55 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 20:38:55 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 20:38:55 INFO Client: Setting up container launch context for our AM\n17/11/25 20:38:55 INFO Client: Setting up the launch environment for our AM container\n17/11/25 20:38:56 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:38:56 INFO Client: Preparing resources for our AM container\n17/11/25 20:38:56 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:38:56 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:38:56 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0018/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 20:38:56 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0018/org.apache.commons_commons-csv-1.1.jar\n17/11/25 20:38:56 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0018/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 20:38:56 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest4/counttest4.py\n17/11/25 20:38:56 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0018/pyspark.zip\n17/11/25 20:38:56 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0018/py4j-0.9-src.zip\n17/11/25 20:38:56 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 20:38:56 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 20:38:56 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 20:38:56 INFO Client: Uploading resource file:/tmp/spark-55197412-6e5a-4188-870d-29363cae7771/__spark_conf__3364750233554043535.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0018/__spark_conf__3364750233554043535.zip\n17/11/25 20:38:56 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 20:38:56 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 20:38:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 20:38:56 INFO Client: Submitting application 18 to ResourceManager\n17/11/25 20:38:56 INFO YarnClientImpl: Submitted application application_1511598448747_0018\n17/11/25 20:38:56 INFO Client: Application report for application_1511598448747_0018 (state: ACCEPTED)\n17/11/25 20:38:56 INFO Client: \n17/11/25 20:38:56 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 20:38:56 INFO ShutdownHookManager: Deleting directory /tmp/spark-55197412-6e5a-4188-870d-29363cae7771\n', '4', '2017-11-25 20:40:12.146791', '2017-11-25 20:38:51.583084', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('23', '17', '23', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 200ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 20:44:04 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511613843801\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0019/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/11/25 20:44:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 20:44:02 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 20:44:02 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 20:44:02 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 20:44:02 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 20:44:02 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 20:44:02 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 20:44:02 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 20:44:02 INFO Client: Setting up container launch context for our AM\n17/11/25 20:44:02 INFO Client: Setting up the launch environment for our AM container\n17/11/25 20:44:02 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:44:02 INFO Client: Preparing resources for our AM container\n17/11/25 20:44:03 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:44:03 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:44:03 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0019/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 20:44:03 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0019/org.apache.commons_commons-csv-1.1.jar\n17/11/25 20:44:03 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0019/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 20:44:03 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest4/counttest4.py\n17/11/25 20:44:03 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0019/pyspark.zip\n17/11/25 20:44:03 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0019/py4j-0.9-src.zip\n17/11/25 20:44:03 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 20:44:03 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 20:44:03 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 20:44:03 INFO Client: Uploading resource file:/tmp/spark-5da2b776-33d2-4691-afa9-f486531cec00/__spark_conf__2617950970428299222.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0019/__spark_conf__2617950970428299222.zip\n17/11/25 20:44:03 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 20:44:03 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 20:44:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 20:44:03 INFO Client: Submitting application 19 to ResourceManager\n17/11/25 20:44:04 INFO YarnClientImpl: Submitted application application_1511598448747_0019\n17/11/25 20:44:04 INFO Client: Application report for application_1511598448747_0019 (state: ACCEPTED)\n17/11/25 20:44:04 INFO Client: \n17/11/25 20:44:04 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 20:44:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-5da2b776-33d2-4691-afa9-f486531cec00\n', '4', '2017-11-25 20:45:19.126416', '2017-11-25 20:43:58.380355', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('24', '17', '-1', '', '', '', '4', null, '2017-11-25 20:52:19.767697', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('25', '17', '-1', '', '', '', '4', null, '2017-11-25 20:53:28.815753', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('26', '17', '-1', '', '', '', '4', null, '2017-11-25 20:54:50.629854', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('27', '17', '24', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 218ms :: artifacts dl 6ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/7ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511614549611\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0020/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/11/25 20:55:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 20:55:48 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 20:55:48 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 20:55:48 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 20:55:48 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 20:55:48 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 20:55:48 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 20:55:48 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 20:55:48 INFO Client: Setting up container launch context for our AM\n17/11/25 20:55:48 INFO Client: Setting up the launch environment for our AM container\n17/11/25 20:55:48 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:55:48 INFO Client: Preparing resources for our AM container\n17/11/25 20:55:48 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:55:48 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:55:48 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0020/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 20:55:49 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0020/org.apache.commons_commons-csv-1.1.jar\n17/11/25 20:55:49 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0020/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 20:55:49 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest4/counttest4.py\n17/11/25 20:55:49 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0020/pyspark.zip\n17/11/25 20:55:49 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0020/py4j-0.9-src.zip\n17/11/25 20:55:49 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 20:55:49 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 20:55:49 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 20:55:49 INFO Client: Uploading resource file:/tmp/spark-5a426aef-5e69-4474-b5bb-08996c92819d/__spark_conf__167013851629286798.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0020/__spark_conf__167013851629286798.zip\n17/11/25 20:55:49 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 20:55:49 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 20:55:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 20:55:49 INFO Client: Submitting application 20 to ResourceManager\n17/11/25 20:55:49 INFO YarnClientImpl: Submitted application application_1511598448747_0020\n17/11/25 20:55:49 INFO Client: Application report for application_1511598448747_0020 (state: ACCEPTED)\n17/11/25 20:55:49 INFO Client: \n17/11/25 20:55:49 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 20:55:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-5a426aef-5e69-4474-b5bb-08996c92819d\n', '4', '2017-11-25 20:57:04.963720', '2017-11-25 20:55:44.397710', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('28', '17', '25', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 198ms :: artifacts dl 4ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/7ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 20:57:38 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511614658803\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0021/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 20:57:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 20:57:37 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 20:57:37 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 20:57:37 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 20:57:37 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 20:57:37 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 20:57:37 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 20:57:37 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 20:57:37 INFO Client: Setting up container launch context for our AM\n17/11/25 20:57:37 INFO Client: Setting up the launch environment for our AM container\n17/11/25 20:57:37 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:57:37 INFO Client: Preparing resources for our AM container\n17/11/25 20:57:38 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:57:38 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 20:57:38 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0021/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 20:57:38 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0021/org.apache.commons_commons-csv-1.1.jar\n17/11/25 20:57:38 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0021/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 20:57:38 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest4/counttest4.py\n17/11/25 20:57:38 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0021/pyspark.zip\n17/11/25 20:57:38 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0021/py4j-0.9-src.zip\n17/11/25 20:57:38 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 20:57:38 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 20:57:38 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 20:57:38 INFO Client: Uploading resource file:/tmp/spark-4513cd52-7a0c-4d5d-a085-e6294dccc897/__spark_conf__1358176784087353636.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0021/__spark_conf__1358176784087353636.zip\n17/11/25 20:57:38 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 20:57:38 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 20:57:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 20:57:38 INFO Client: Submitting application 21 to ResourceManager\n17/11/25 20:57:39 INFO YarnClientImpl: Submitted application application_1511598448747_0021\n17/11/25 20:57:39 INFO Client: Application report for application_1511598448747_0021 (state: ACCEPTED)\n17/11/25 20:57:39 INFO Client: \n17/11/25 20:57:39 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 20:57:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-4513cd52-7a0c-4d5d-a085-e6294dccc897\n', '4', '2017-11-25 20:58:19.134686', '2017-11-25 20:57:33.389850', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('29', '18', '26', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 199ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511616939218\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0022/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 21:35:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 21:35:37 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 21:35:37 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 21:35:37 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 21:35:38 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 21:35:38 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 21:35:38 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 21:35:38 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 21:35:38 INFO Client: Setting up container launch context for our AM\n17/11/25 21:35:38 INFO Client: Setting up the launch environment for our AM container\n17/11/25 21:35:38 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:35:38 INFO Client: Preparing resources for our AM container\n17/11/25 21:35:38 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:35:38 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:35:38 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0022/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 21:35:38 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0022/org.apache.commons_commons-csv-1.1.jar\n17/11/25 21:35:38 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0022/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 21:35:38 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest5/counttest5.py\n17/11/25 21:35:38 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0022/pyspark.zip\n17/11/25 21:35:38 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0022/py4j-0.9-src.zip\n17/11/25 21:35:38 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 21:35:38 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 21:35:38 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 21:35:38 INFO Client: Uploading resource file:/tmp/spark-8b4f32cf-339e-41ab-bb21-53d1f02b768b/__spark_conf__8121119145691206706.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0022/__spark_conf__8121119145691206706.zip\n17/11/25 21:35:39 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 21:35:39 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 21:35:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 21:35:39 INFO Client: Submitting application 22 to ResourceManager\n17/11/25 21:35:39 INFO YarnClientImpl: Submitted application application_1511598448747_0022\n17/11/25 21:35:39 INFO Client: Application report for application_1511598448747_0022 (state: ACCEPTED)\n17/11/25 21:35:39 INFO Client: \n17/11/25 21:35:39 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 21:35:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-8b4f32cf-339e-41ab-bb21-53d1f02b768b\n', '4', '2017-11-25 21:36:24.536179', '2017-11-25 21:35:33.617005', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('30', '6', '27', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 263ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 21:49:28 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511617768274\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0023/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 21:49:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 21:49:26 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 21:49:26 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 21:49:26 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 21:49:27 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 21:49:27 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 21:49:27 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 21:49:27 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 21:49:27 INFO Client: Setting up container launch context for our AM\n17/11/25 21:49:27 INFO Client: Setting up the launch environment for our AM container\n17/11/25 21:49:27 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:49:27 INFO Client: Preparing resources for our AM container\n17/11/25 21:49:27 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:49:27 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:49:27 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0023/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 21:49:27 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0023/org.apache.commons_commons-csv-1.1.jar\n17/11/25 21:49:27 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0023/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 21:49:27 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/user/yufeng/hdp/code_file/pi/pi.py\n17/11/25 21:49:27 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0023/pyspark.zip\n17/11/25 21:49:27 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0023/py4j-0.9-src.zip\n17/11/25 21:49:28 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 21:49:28 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 21:49:28 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 21:49:28 INFO Client: Uploading resource file:/tmp/spark-c05a5106-7d30-4853-8079-553a79e06ed5/__spark_conf__1782312260819310539.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0023/__spark_conf__1782312260819310539.zip\n17/11/25 21:49:28 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 21:49:28 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 21:49:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 21:49:28 INFO Client: Submitting application 23 to ResourceManager\n17/11/25 21:49:28 INFO YarnClientImpl: Submitted application application_1511598448747_0023\n17/11/25 21:49:28 INFO Client: Application report for application_1511598448747_0023 (state: ACCEPTED)\n17/11/25 21:49:28 INFO Client: \n17/11/25 21:49:28 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 21:49:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-c05a5106-7d30-4853-8079-553a79e06ed5\n', '4', '2017-11-25 21:50:08.604626', '2017-11-25 21:49:22.812434', null, 'import time\nfrom pyspark import SparkContext\nimport random\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('31', '6', '28', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 196ms :: artifacts dl 4ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/5ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511617968842\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0024/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 21:52:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 21:52:47 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 21:52:47 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 21:52:47 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 21:52:47 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 21:52:47 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 21:52:47 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 21:52:47 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 21:52:47 INFO Client: Setting up container launch context for our AM\n17/11/25 21:52:47 INFO Client: Setting up the launch environment for our AM container\n17/11/25 21:52:47 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:52:47 INFO Client: Preparing resources for our AM container\n17/11/25 21:52:48 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:52:48 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:52:48 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0024/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 21:52:48 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0024/org.apache.commons_commons-csv-1.1.jar\n17/11/25 21:52:48 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0024/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 21:52:48 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/user/yufeng/hdp/code_file/pi/pi.py\n17/11/25 21:52:48 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0024/pyspark.zip\n17/11/25 21:52:48 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0024/py4j-0.9-src.zip\n17/11/25 21:52:48 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 21:52:48 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 21:52:48 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 21:52:48 INFO Client: Uploading resource file:/tmp/spark-82a08e02-838d-4549-9713-004bd32368a1/__spark_conf__8795161964179650856.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0024/__spark_conf__8795161964179650856.zip\n17/11/25 21:52:48 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 21:52:48 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 21:52:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 21:52:48 INFO Client: Submitting application 24 to ResourceManager\n17/11/25 21:52:49 INFO YarnClientImpl: Submitted application application_1511598448747_0024\n17/11/25 21:52:49 INFO Client: Application report for application_1511598448747_0024 (state: ACCEPTED)\n17/11/25 21:52:49 INFO Client: \n17/11/25 21:52:49 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 21:52:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-82a08e02-838d-4549-9713-004bd32368a1\n', '4', '2017-11-25 21:53:29.163843', '2017-11-25 21:52:43.481213', null, 'import time\nfrom pyspark import SparkContext\nimport random\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('32', '6', '29', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 205ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511618172648\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0025/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 21:56:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 21:56:11 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 21:56:11 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 21:56:11 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 21:56:11 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 21:56:11 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 21:56:11 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 21:56:11 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 21:56:11 INFO Client: Setting up container launch context for our AM\n17/11/25 21:56:11 INFO Client: Setting up the launch environment for our AM container\n17/11/25 21:56:11 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:56:11 INFO Client: Preparing resources for our AM container\n17/11/25 21:56:11 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:56:11 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:56:11 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0025/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 21:56:12 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0025/org.apache.commons_commons-csv-1.1.jar\n17/11/25 21:56:12 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0025/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 21:56:12 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/user/yufeng/hdp/code_file/pi/pi.py\n17/11/25 21:56:12 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0025/pyspark.zip\n17/11/25 21:56:12 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0025/py4j-0.9-src.zip\n17/11/25 21:56:12 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 21:56:12 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 21:56:12 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 21:56:12 INFO Client: Uploading resource file:/tmp/spark-d939c4fd-cd43-441c-bbe6-cb5a80c62c4d/__spark_conf__4108466170744424502.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0025/__spark_conf__4108466170744424502.zip\n17/11/25 21:56:12 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 21:56:12 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 21:56:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 21:56:12 INFO Client: Submitting application 25 to ResourceManager\n17/11/25 21:56:12 INFO YarnClientImpl: Submitted application application_1511598448747_0025\n17/11/25 21:56:12 INFO Client: Application report for application_1511598448747_0025 (state: ACCEPTED)\n17/11/25 21:56:12 INFO Client: \n17/11/25 21:56:12 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 21:56:12 INFO ShutdownHookManager: Deleting directory /tmp/spark-d939c4fd-cd43-441c-bbe6-cb5a80c62c4d\n', '4', '2017-11-25 21:56:53.015786', '2017-11-25 21:56:07.295620', null, 'import time\nfrom pyspark import SparkContext\nimport random\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()');
INSERT INTO `execution` VALUES ('33', '17', '30', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 202ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 21:58:23 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511618303871\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0026/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 21:58:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 21:58:22 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 21:58:22 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 21:58:22 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 21:58:22 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 21:58:23 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 21:58:23 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 21:58:23 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 21:58:23 INFO Client: Setting up container launch context for our AM\n17/11/25 21:58:23 INFO Client: Setting up the launch environment for our AM container\n17/11/25 21:58:23 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:58:23 INFO Client: Preparing resources for our AM container\n17/11/25 21:58:23 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:58:23 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 21:58:23 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0026/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 21:58:23 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0026/org.apache.commons_commons-csv-1.1.jar\n17/11/25 21:58:23 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0026/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 21:58:23 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest4/counttest4.py\n17/11/25 21:58:23 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0026/pyspark.zip\n17/11/25 21:58:23 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0026/py4j-0.9-src.zip\n17/11/25 21:58:23 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 21:58:23 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 21:58:23 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 21:58:23 INFO Client: Uploading resource file:/tmp/spark-ed186c52-c3bc-44b5-8d2e-1153d4868d9d/__spark_conf__1717241274007979709.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0026/__spark_conf__1717241274007979709.zip\n17/11/25 21:58:23 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 21:58:23 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 21:58:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 21:58:23 INFO Client: Submitting application 26 to ResourceManager\n17/11/25 21:58:24 INFO YarnClientImpl: Submitted application application_1511598448747_0026\n17/11/25 21:58:24 INFO Client: Application report for application_1511598448747_0026 (state: ACCEPTED)\n17/11/25 21:58:24 INFO Client: \n17/11/25 21:58:24 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 21:58:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-ed186c52-c3bc-44b5-8d2e-1153d4868d9d\n', '4', '2017-11-25 21:59:04.202674', '2017-11-25 21:58:18.601850', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('34', '18', '31', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 195ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/5ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 22:20:18 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511619618330\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0027/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/11/25 22:20:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 22:20:16 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 22:20:16 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 22:20:16 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 22:20:17 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 22:20:17 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 22:20:17 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 22:20:17 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 22:20:17 INFO Client: Setting up container launch context for our AM\n17/11/25 22:20:17 INFO Client: Setting up the launch environment for our AM container\n17/11/25 22:20:17 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:20:17 INFO Client: Preparing resources for our AM container\n17/11/25 22:20:17 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:20:17 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:20:17 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0027/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 22:20:17 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0027/org.apache.commons_commons-csv-1.1.jar\n17/11/25 22:20:17 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0027/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 22:20:17 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest5/counttest5.py\n17/11/25 22:20:17 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0027/pyspark.zip\n17/11/25 22:20:18 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0027/py4j-0.9-src.zip\n17/11/25 22:20:18 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 22:20:18 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 22:20:18 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 22:20:18 INFO Client: Uploading resource file:/tmp/spark-61ecea8e-4637-4e5b-847a-7bc090d3bfb8/__spark_conf__2893319048495759479.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0027/__spark_conf__2893319048495759479.zip\n17/11/25 22:20:18 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 22:20:18 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 22:20:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 22:20:18 INFO Client: Submitting application 27 to ResourceManager\n17/11/25 22:20:18 INFO YarnClientImpl: Submitted application application_1511598448747_0027\n17/11/25 22:20:18 INFO Client: Application report for application_1511598448747_0027 (state: ACCEPTED)\n17/11/25 22:20:18 INFO Client: \n17/11/25 22:20:18 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 22:20:18 INFO ShutdownHookManager: Deleting directory /tmp/spark-61ecea8e-4637-4e5b-847a-7bc090d3bfb8\n', '4', '2017-11-25 22:21:33.742226', '2017-11-25 22:20:12.882454', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('35', '18', '32', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 193ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511620159599\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0028/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 22:29:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 22:29:18 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 22:29:18 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 22:29:18 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 22:29:18 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 22:29:18 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 22:29:18 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 22:29:18 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 22:29:18 INFO Client: Setting up container launch context for our AM\n17/11/25 22:29:18 INFO Client: Setting up the launch environment for our AM container\n17/11/25 22:29:18 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:29:18 INFO Client: Preparing resources for our AM container\n17/11/25 22:29:18 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:29:18 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:29:18 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0028/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 22:29:19 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0028/org.apache.commons_commons-csv-1.1.jar\n17/11/25 22:29:19 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0028/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 22:29:19 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest5/counttest5.py\n17/11/25 22:29:19 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0028/pyspark.zip\n17/11/25 22:29:19 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0028/py4j-0.9-src.zip\n17/11/25 22:29:19 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 22:29:19 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 22:29:19 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 22:29:19 INFO Client: Uploading resource file:/tmp/spark-ec352d0b-ba3d-4d6f-ae5b-5522835a581c/__spark_conf__8356497809546395729.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0028/__spark_conf__8356497809546395729.zip\n17/11/25 22:29:19 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 22:29:19 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 22:29:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 22:29:19 INFO Client: Submitting application 28 to ResourceManager\n17/11/25 22:29:19 INFO YarnClientImpl: Submitted application application_1511598448747_0028\n17/11/25 22:29:19 INFO Client: Application report for application_1511598448747_0028 (state: ACCEPTED)\n17/11/25 22:29:19 INFO Client: \n17/11/25 22:29:19 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 22:29:19 INFO ShutdownHookManager: Deleting directory /tmp/spark-ec352d0b-ba3d-4d6f-ae5b-5522835a581c\n', '4', '2017-11-25 22:30:04.978843', '2017-11-25 22:29:13.991883', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('36', '18', '33', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 210ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/5ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 22:31:16 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511620276699\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0029/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 22:31:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 22:31:15 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 22:31:15 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 22:31:15 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 22:31:15 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 22:31:15 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 22:31:15 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 22:31:15 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 22:31:15 INFO Client: Setting up container launch context for our AM\n17/11/25 22:31:15 INFO Client: Setting up the launch environment for our AM container\n17/11/25 22:31:15 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:31:15 INFO Client: Preparing resources for our AM container\n17/11/25 22:31:16 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:31:16 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:31:16 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0029/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 22:31:16 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0029/org.apache.commons_commons-csv-1.1.jar\n17/11/25 22:31:16 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0029/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 22:31:16 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest5/counttest5.py\n17/11/25 22:31:16 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0029/pyspark.zip\n17/11/25 22:31:16 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0029/py4j-0.9-src.zip\n17/11/25 22:31:16 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 22:31:16 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 22:31:16 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 22:31:16 INFO Client: Uploading resource file:/tmp/spark-7f5a87c7-2014-49d3-a0e4-d55d65c57a84/__spark_conf__7395589589407787835.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0029/__spark_conf__7395589589407787835.zip\n17/11/25 22:31:16 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 22:31:16 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 22:31:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 22:31:16 INFO Client: Submitting application 29 to ResourceManager\n17/11/25 22:31:16 INFO YarnClientImpl: Submitted application application_1511598448747_0029\n17/11/25 22:31:16 INFO Client: Application report for application_1511598448747_0029 (state: ACCEPTED)\n17/11/25 22:31:16 INFO Client: \n17/11/25 22:31:16 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 22:31:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-7f5a87c7-2014-49d3-a0e4-d55d65c57a84\n', '4', '2017-11-25 22:31:57.049634', '2017-11-25 22:31:11.236466', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('37', '18', '34', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 249ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/7ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 22:34:08 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511620448703\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0030/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 22:34:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 22:34:07 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 22:34:07 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 22:34:07 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 22:34:07 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 22:34:07 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 22:34:07 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 22:34:07 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 22:34:07 INFO Client: Setting up container launch context for our AM\n17/11/25 22:34:07 INFO Client: Setting up the launch environment for our AM container\n17/11/25 22:34:07 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:34:07 INFO Client: Preparing resources for our AM container\n17/11/25 22:34:07 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:34:07 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:34:07 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0030/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 22:34:08 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0030/org.apache.commons_commons-csv-1.1.jar\n17/11/25 22:34:08 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0030/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 22:34:08 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest5/counttest5.py\n17/11/25 22:34:08 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0030/pyspark.zip\n17/11/25 22:34:08 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0030/py4j-0.9-src.zip\n17/11/25 22:34:08 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 22:34:08 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 22:34:08 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 22:34:08 INFO Client: Uploading resource file:/tmp/spark-9d44665c-3b63-46e3-94b8-fd0e6fb24a34/__spark_conf__3561034987722632927.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0030/__spark_conf__3561034987722632927.zip\n17/11/25 22:34:08 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 22:34:08 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 22:34:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 22:34:08 INFO Client: Submitting application 30 to ResourceManager\n17/11/25 22:34:08 INFO YarnClientImpl: Submitted application application_1511598448747_0030\n17/11/25 22:34:08 INFO Client: Application report for application_1511598448747_0030 (state: ACCEPTED)\n17/11/25 22:34:08 INFO Client: \n17/11/25 22:34:08 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 22:34:08 INFO ShutdownHookManager: Deleting directory /tmp/spark-9d44665c-3b63-46e3-94b8-fd0e6fb24a34\n', '4', '2017-11-25 22:34:20.338320', '2017-11-25 22:34:02.939068', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('38', '18', '35', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 222ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511620451189\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0031/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 22:34:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 22:34:09 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 22:34:09 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 22:34:09 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 22:34:09 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 22:34:09 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 22:34:10 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 22:34:10 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 22:34:10 INFO Client: Setting up container launch context for our AM\n17/11/25 22:34:10 INFO Client: Setting up the launch environment for our AM container\n17/11/25 22:34:10 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:34:10 INFO Client: Preparing resources for our AM container\n17/11/25 22:34:10 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:34:10 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:34:10 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0031/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 22:34:10 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0031/org.apache.commons_commons-csv-1.1.jar\n17/11/25 22:34:10 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0031/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 22:34:10 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest5/counttest5.py\n17/11/25 22:34:10 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0031/pyspark.zip\n17/11/25 22:34:10 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0031/py4j-0.9-src.zip\n17/11/25 22:34:10 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 22:34:10 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 22:34:10 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 22:34:10 INFO Client: Uploading resource file:/tmp/spark-d95e3535-ac66-49b7-a023-dcb8a91e8e07/__spark_conf__3158600983933485576.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0031/__spark_conf__3158600983933485576.zip\n17/11/25 22:34:11 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 22:34:11 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 22:34:11 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 22:34:11 INFO Client: Submitting application 31 to ResourceManager\n17/11/25 22:34:11 INFO YarnClientImpl: Submitted application application_1511598448747_0031\n17/11/25 22:34:11 INFO Client: Application report for application_1511598448747_0031 (state: ACCEPTED)\n17/11/25 22:34:11 INFO Client: \n17/11/25 22:34:11 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 22:34:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-d95e3535-ac66-49b7-a023-dcb8a91e8e07\n', '4', '2017-11-25 22:34:20.100489', '2017-11-25 22:34:05.127305', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('39', '18', '36', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 234ms :: artifacts dl 7ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/10ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511620592126\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0032/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 22:36:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 22:36:30 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 22:36:30 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 22:36:30 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 22:36:31 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 22:36:31 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 22:36:31 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 22:36:31 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 22:36:31 INFO Client: Setting up container launch context for our AM\n17/11/25 22:36:31 INFO Client: Setting up the launch environment for our AM container\n17/11/25 22:36:31 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:36:31 INFO Client: Preparing resources for our AM container\n17/11/25 22:36:31 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:36:31 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:36:31 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0032/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 22:36:31 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0032/org.apache.commons_commons-csv-1.1.jar\n17/11/25 22:36:31 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0032/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 22:36:31 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest5/counttest5.py\n17/11/25 22:36:31 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0032/pyspark.zip\n17/11/25 22:36:31 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0032/py4j-0.9-src.zip\n17/11/25 22:36:31 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 22:36:31 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 22:36:31 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 22:36:31 INFO Client: Uploading resource file:/tmp/spark-c578e778-a0fd-4dce-81b1-a77066b3e2a7/__spark_conf__2304434165369212626.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0032/__spark_conf__2304434165369212626.zip\n17/11/25 22:36:31 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 22:36:31 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 22:36:31 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 22:36:32 INFO Client: Submitting application 32 to ResourceManager\n17/11/25 22:36:32 INFO YarnClientImpl: Submitted application application_1511598448747_0032\n17/11/25 22:36:32 INFO Client: Application report for application_1511598448747_0032 (state: ACCEPTED)\n17/11/25 22:36:32 INFO Client: \n17/11/25 22:36:32 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 22:36:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-c578e778-a0fd-4dce-81b1-a77066b3e2a7\n', '4', '2017-11-25 22:37:12.473446', '2017-11-25 22:36:26.251429', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('40', '18', '37', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 208ms :: artifacts dl 7ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/9ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 22:40:45 +0800 2017] Scheduler has assigned a container for AM, waiting for AM container to be launched\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511620844955\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0033/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 22:40:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 22:40:43 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 22:40:43 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 22:40:43 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 22:40:43 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 22:40:44 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 22:40:44 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 22:40:44 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 22:40:44 INFO Client: Setting up container launch context for our AM\n17/11/25 22:40:44 INFO Client: Setting up the launch environment for our AM container\n17/11/25 22:40:44 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:40:44 INFO Client: Preparing resources for our AM container\n17/11/25 22:40:44 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:40:44 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:40:44 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0033/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 22:40:44 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0033/org.apache.commons_commons-csv-1.1.jar\n17/11/25 22:40:44 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0033/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 22:40:44 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest5/counttest5.py\n17/11/25 22:40:44 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0033/pyspark.zip\n17/11/25 22:40:44 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0033/py4j-0.9-src.zip\n17/11/25 22:40:44 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 22:40:44 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 22:40:44 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 22:40:44 INFO Client: Uploading resource file:/tmp/spark-09961bd2-de75-4d90-bdbc-2144a3d66ce0/__spark_conf__597557393257665028.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0033/__spark_conf__597557393257665028.zip\n17/11/25 22:40:44 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 22:40:44 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 22:40:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 22:40:44 INFO Client: Submitting application 33 to ResourceManager\n17/11/25 22:40:45 INFO YarnClientImpl: Submitted application application_1511598448747_0033\n17/11/25 22:40:45 INFO Client: Application report for application_1511598448747_0033 (state: ACCEPTED)\n17/11/25 22:40:45 INFO Client: \n17/11/25 22:40:45 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 22:40:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-09961bd2-de75-4d90-bdbc-2144a3d66ce0\n', '4', '2017-11-25 22:41:25.307398', '2017-11-25 22:40:39.310580', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('41', '18', '38', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 188ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/5ms)\n	 client token: N/A\n	 diagnostics: [星期六 十一月 25 22:46:04 +0800 2017] Scheduler has assigned a container for AM, waiting for AM container to be launched\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511621164039\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0034/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/11/25 22:46:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/25 22:46:02 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/25 22:46:02 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/25 22:46:02 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/25 22:46:03 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/25 22:46:03 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/25 22:46:03 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/25 22:46:03 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/25 22:46:03 INFO Client: Setting up container launch context for our AM\n17/11/25 22:46:03 INFO Client: Setting up the launch environment for our AM container\n17/11/25 22:46:03 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:46:03 INFO Client: Preparing resources for our AM container\n17/11/25 22:46:03 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:46:03 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/25 22:46:03 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0034/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/25 22:46:03 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0034/org.apache.commons_commons-csv-1.1.jar\n17/11/25 22:46:03 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0034/com.univocity_univocity-parsers-1.5.1.jar\n17/11/25 22:46:03 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest5/counttest5.py\n17/11/25 22:46:03 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0034/pyspark.zip\n17/11/25 22:46:03 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0034/py4j-0.9-src.zip\n17/11/25 22:46:03 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/25 22:46:03 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/25 22:46:03 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/25 22:46:03 INFO Client: Uploading resource file:/tmp/spark-a0d35e8c-0f0b-4d15-bc5e-0d9a9005c6bb/__spark_conf__5206286256016319024.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0034/__spark_conf__5206286256016319024.zip\n17/11/25 22:46:03 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/25 22:46:03 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/25 22:46:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/25 22:46:04 INFO Client: Submitting application 34 to ResourceManager\n17/11/25 22:46:04 INFO YarnClientImpl: Submitted application application_1511598448747_0034\n17/11/25 22:46:04 INFO Client: Application report for application_1511598448747_0034 (state: ACCEPTED)\n17/11/25 22:46:04 INFO Client: \n17/11/25 22:46:04 INFO ShutdownHookManager: Shutdown hook called\n17/11/25 22:46:04 INFO ShutdownHookManager: Deleting directory /tmp/spark-a0d35e8c-0f0b-4d15-bc5e-0d9a9005c6bb\n', '4', '2017-11-25 22:46:44.391523', '2017-11-25 22:45:58.749952', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('42', '19', '39', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n:: resolution report :: resolve 204ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 3 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1511692832314\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0042/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/11/26 18:40:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/11/26 18:40:30 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/11/26 18:40:30 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/11/26 18:40:30 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/11/26 18:40:31 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/11/26 18:40:31 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/11/26 18:40:31 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/11/26 18:40:31 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/11/26 18:40:31 INFO Client: Setting up container launch context for our AM\n17/11/26 18:40:31 INFO Client: Setting up the launch environment for our AM container\n17/11/26 18:40:31 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/26 18:40:31 INFO Client: Preparing resources for our AM container\n17/11/26 18:40:31 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/26 18:40:31 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/11/26 18:40:31 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0042/com.databricks_spark-csv_2.10-1.5.0.jar\n17/11/26 18:40:31 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0042/org.apache.commons_commons-csv-1.1.jar\n17/11/26 18:40:31 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0042/com.univocity_univocity-parsers-1.5.1.jar\n17/11/26 18:40:31 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/cad/code_file/counttest5/counttest5.py\n17/11/26 18:40:31 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0042/pyspark.zip\n17/11/26 18:40:32 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0042/py4j-0.9-src.zip\n17/11/26 18:40:32 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/11/26 18:40:32 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/11/26 18:40:32 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/11/26 18:40:32 INFO Client: Uploading resource file:/tmp/spark-fcfca43e-84bd-4d68-99e0-359051b83225/__spark_conf__542300077528659530.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0042/__spark_conf__542300077528659530.zip\n17/11/26 18:40:32 INFO SecurityManager: Changing view acls to: root,hdfs\n17/11/26 18:40:32 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/11/26 18:40:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/11/26 18:40:32 INFO Client: Submitting application 42 to ResourceManager\n17/11/26 18:40:32 INFO YarnClientImpl: Submitted application application_1511598448747_0042\n17/11/26 18:40:32 INFO Client: Application report for application_1511598448747_0042 (state: ACCEPTED)\n17/11/26 18:40:32 INFO Client: \n17/11/26 18:40:32 INFO ShutdownHookManager: Shutdown hook called\n17/11/26 18:40:32 INFO ShutdownHookManager: Deleting directory /tmp/spark-fcfca43e-84bd-4d68-99e0-359051b83225\n', '9', '2017-11-26 18:41:57.692363', '2017-11-26 18:40:26.626802', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('43', '20', '40', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\nmysql#mysql-connector-java added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n	found mysql#mysql-connector-java;5.1.39 in central\n:: resolution report :: resolve 217ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	mysql#mysql-connector-java;5.1.39 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 4 already retrieved (0kB/9ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1512522784800\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0050/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/12/06 09:13:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/12/06 09:13:02 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/12/06 09:13:03 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/12/06 09:13:03 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/12/06 09:13:03 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/12/06 09:13:03 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/12/06 09:13:03 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/12/06 09:13:03 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/12/06 09:13:03 INFO Client: Setting up container launch context for our AM\n17/12/06 09:13:03 INFO Client: Setting up the launch environment for our AM container\n17/12/06 09:13:03 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:13:03 INFO Client: Preparing resources for our AM container\n17/12/06 09:13:03 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:13:03 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:13:03 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0050/com.databricks_spark-csv_2.10-1.5.0.jar\n17/12/06 09:13:04 INFO Client: Uploading resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0050/mysql_mysql-connector-java-5.1.39.jar\n17/12/06 09:13:04 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0050/org.apache.commons_commons-csv-1.1.jar\n17/12/06 09:13:04 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0050/com.univocity_univocity-parsers-1.5.1.jar\n17/12/06 09:13:04 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/t_hdp/code_file/counttest5/counttest5.py\n17/12/06 09:13:04 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0050/pyspark.zip\n17/12/06 09:13:04 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0050/py4j-0.9-src.zip\n17/12/06 09:13:04 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/12/06 09:13:04 WARN Client: Resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar added multiple times to distributed cache.\n17/12/06 09:13:04 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/12/06 09:13:04 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/12/06 09:13:04 INFO Client: Uploading resource file:/tmp/spark-69264956-30e5-4276-844a-d18ec05bb565/__spark_conf__4914305916511628908.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0050/__spark_conf__4914305916511628908.zip\n17/12/06 09:13:04 INFO SecurityManager: Changing view acls to: root,hdfs\n17/12/06 09:13:04 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/12/06 09:13:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/12/06 09:13:04 INFO Client: Submitting application 50 to ResourceManager\n17/12/06 09:13:05 INFO YarnClientImpl: Submitted application application_1511598448747_0050\n17/12/06 09:13:05 INFO Client: Application report for application_1511598448747_0050 (state: ACCEPTED)\n17/12/06 09:13:05 INFO Client: \n17/12/06 09:13:05 INFO ShutdownHookManager: Shutdown hook called\n17/12/06 09:13:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-69264956-30e5-4276-844a-d18ec05bb565\n', '10', '2017-12-06 09:14:30.143645', '2017-12-06 09:12:59.251965', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('44', '20', '41', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\nmysql#mysql-connector-java added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n	found mysql#mysql-connector-java;5.1.39 in central\n:: resolution report :: resolve 224ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	mysql#mysql-connector-java;5.1.39 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 4 already retrieved (0kB/7ms)\n	 client token: N/A\n	 diagnostics: [星期三 十二月 06 09:23:23 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1512523403549\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0051/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/12/06 09:23:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/12/06 09:23:21 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/12/06 09:23:21 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/12/06 09:23:22 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/12/06 09:23:22 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/12/06 09:23:22 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/12/06 09:23:22 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/12/06 09:23:22 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/12/06 09:23:22 INFO Client: Setting up container launch context for our AM\n17/12/06 09:23:22 INFO Client: Setting up the launch environment for our AM container\n17/12/06 09:23:22 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:23:22 INFO Client: Preparing resources for our AM container\n17/12/06 09:23:22 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:23:22 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:23:22 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0051/com.databricks_spark-csv_2.10-1.5.0.jar\n17/12/06 09:23:22 INFO Client: Uploading resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0051/mysql_mysql-connector-java-5.1.39.jar\n17/12/06 09:23:23 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0051/org.apache.commons_commons-csv-1.1.jar\n17/12/06 09:23:23 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0051/com.univocity_univocity-parsers-1.5.1.jar\n17/12/06 09:23:23 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/t_hdp/code_file/counttest5/counttest5.py\n17/12/06 09:23:23 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0051/pyspark.zip\n17/12/06 09:23:23 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0051/py4j-0.9-src.zip\n17/12/06 09:23:23 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/12/06 09:23:23 WARN Client: Resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar added multiple times to distributed cache.\n17/12/06 09:23:23 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/12/06 09:23:23 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/12/06 09:23:23 INFO Client: Uploading resource file:/tmp/spark-d0dfc3ee-aed7-4599-9d4c-b46f0d3fb3d5/__spark_conf__8163346927210774600.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0051/__spark_conf__8163346927210774600.zip\n17/12/06 09:23:23 INFO SecurityManager: Changing view acls to: root,hdfs\n17/12/06 09:23:23 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/12/06 09:23:23 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/12/06 09:23:23 INFO Client: Submitting application 51 to ResourceManager\n17/12/06 09:23:23 INFO YarnClientImpl: Submitted application application_1511598448747_0051\n17/12/06 09:23:23 INFO Client: Application report for application_1511598448747_0051 (state: ACCEPTED)\n17/12/06 09:23:23 INFO Client: \n17/12/06 09:23:23 INFO ShutdownHookManager: Shutdown hook called\n17/12/06 09:23:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-d0dfc3ee-aed7-4599-9d4c-b46f0d3fb3d5\n', '10', '2017-12-06 09:24:48.923395', '2017-12-06 09:23:18.126742', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('45', '20', '42', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\nmysql#mysql-connector-java added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n	found mysql#mysql-connector-java;5.1.39 in central\n:: resolution report :: resolve 213ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	mysql#mysql-connector-java;5.1.39 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 4 already retrieved (0kB/7ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1512523607100\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0052/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/12/06 09:26:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/12/06 09:26:45 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/12/06 09:26:45 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/12/06 09:26:45 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/12/06 09:26:45 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/12/06 09:26:46 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/12/06 09:26:46 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/12/06 09:26:46 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/12/06 09:26:46 INFO Client: Setting up container launch context for our AM\n17/12/06 09:26:46 INFO Client: Setting up the launch environment for our AM container\n17/12/06 09:26:46 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:26:46 INFO Client: Preparing resources for our AM container\n17/12/06 09:26:46 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:26:46 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:26:46 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0052/com.databricks_spark-csv_2.10-1.5.0.jar\n17/12/06 09:26:46 INFO Client: Uploading resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0052/mysql_mysql-connector-java-5.1.39.jar\n17/12/06 09:26:46 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0052/org.apache.commons_commons-csv-1.1.jar\n17/12/06 09:26:46 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0052/com.univocity_univocity-parsers-1.5.1.jar\n17/12/06 09:26:46 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/t_hdp/code_file/counttest5/counttest5.py\n17/12/06 09:26:46 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0052/pyspark.zip\n17/12/06 09:26:46 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0052/py4j-0.9-src.zip\n17/12/06 09:26:46 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/12/06 09:26:46 WARN Client: Resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar added multiple times to distributed cache.\n17/12/06 09:26:46 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/12/06 09:26:46 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/12/06 09:26:46 INFO Client: Uploading resource file:/tmp/spark-051eff62-ae7d-4b56-a7c1-662013578c4b/__spark_conf__3525688210444582113.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0052/__spark_conf__3525688210444582113.zip\n17/12/06 09:26:46 INFO SecurityManager: Changing view acls to: root,hdfs\n17/12/06 09:26:46 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/12/06 09:26:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/12/06 09:26:47 INFO Client: Submitting application 52 to ResourceManager\n17/12/06 09:26:47 INFO YarnClientImpl: Submitted application application_1511598448747_0052\n17/12/06 09:26:47 INFO Client: Application report for application_1511598448747_0052 (state: ACCEPTED)\n17/12/06 09:26:47 INFO Client: \n17/12/06 09:26:47 INFO ShutdownHookManager: Shutdown hook called\n17/12/06 09:26:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-051eff62-ae7d-4b56-a7c1-662013578c4b\n', '10', '2017-12-06 09:28:12.449093', '2017-12-06 09:26:41.642944', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('46', '20', '-1', '', '', '', '10', null, '2017-12-06 09:33:26.838752', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('47', '21', '43', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\nmysql#mysql-connector-java added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n	found mysql#mysql-connector-java;5.1.39 in central\n:: resolution report :: resolve 216ms :: artifacts dl 6ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	mysql#mysql-connector-java;5.1.39 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 4 already retrieved (0kB/7ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1512524283492\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0053/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/12/06 09:38:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/12/06 09:38:01 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/12/06 09:38:01 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/12/06 09:38:02 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/12/06 09:38:02 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/12/06 09:38:02 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/12/06 09:38:02 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/12/06 09:38:02 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/12/06 09:38:02 INFO Client: Setting up container launch context for our AM\n17/12/06 09:38:02 INFO Client: Setting up the launch environment for our AM container\n17/12/06 09:38:02 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:38:02 INFO Client: Preparing resources for our AM container\n17/12/06 09:38:02 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:38:02 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:38:02 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0053/com.databricks_spark-csv_2.10-1.5.0.jar\n17/12/06 09:38:02 INFO Client: Uploading resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0053/mysql_mysql-connector-java-5.1.39.jar\n17/12/06 09:38:02 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0053/org.apache.commons_commons-csv-1.1.jar\n17/12/06 09:38:03 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0053/com.univocity_univocity-parsers-1.5.1.jar\n17/12/06 09:38:03 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/t_hdp/code_file/counttest4/counttest4.py\n17/12/06 09:38:03 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0053/pyspark.zip\n17/12/06 09:38:03 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0053/py4j-0.9-src.zip\n17/12/06 09:38:03 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/12/06 09:38:03 WARN Client: Resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar added multiple times to distributed cache.\n17/12/06 09:38:03 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/12/06 09:38:03 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/12/06 09:38:03 INFO Client: Uploading resource file:/tmp/spark-8c7efec9-a8fe-4d1e-b4d6-729eee1f2549/__spark_conf__6369793895969439274.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0053/__spark_conf__6369793895969439274.zip\n17/12/06 09:38:03 INFO SecurityManager: Changing view acls to: root,hdfs\n17/12/06 09:38:03 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/12/06 09:38:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/12/06 09:38:03 INFO Client: Submitting application 53 to ResourceManager\n17/12/06 09:38:03 INFO YarnClientImpl: Submitted application application_1511598448747_0053\n17/12/06 09:38:03 INFO Client: Application report for application_1511598448747_0053 (state: ACCEPTED)\n17/12/06 09:38:03 INFO Client: \n17/12/06 09:38:03 INFO ShutdownHookManager: Shutdown hook called\n17/12/06 09:38:03 INFO ShutdownHookManager: Deleting directory /tmp/spark-8c7efec9-a8fe-4d1e-b4d6-729eee1f2549\n', '10', '2017-12-06 09:39:28.866126', '2017-12-06 09:37:58.187786', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('48', '20', '44', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\nmysql#mysql-connector-java added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n	found mysql#mysql-connector-java;5.1.39 in central\n:: resolution report :: resolve 223ms :: artifacts dl 6ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	mysql#mysql-connector-java;5.1.39 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 4 already retrieved (0kB/7ms)\n	 client token: N/A\n	 diagnostics: [星期三 十二月 06 09:41:34 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1512524494529\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0054/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \nYARN Diagnostics:\nUser application exited with status 1\n', '17/12/06 09:41:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/12/06 09:41:32 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/12/06 09:41:33 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/12/06 09:41:33 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/12/06 09:41:33 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/12/06 09:41:33 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/12/06 09:41:33 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/12/06 09:41:33 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/12/06 09:41:33 INFO Client: Setting up container launch context for our AM\n17/12/06 09:41:33 INFO Client: Setting up the launch environment for our AM container\n17/12/06 09:41:33 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:41:33 INFO Client: Preparing resources for our AM container\n17/12/06 09:41:33 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:41:33 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:41:33 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0054/com.databricks_spark-csv_2.10-1.5.0.jar\n17/12/06 09:41:34 INFO Client: Uploading resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0054/mysql_mysql-connector-java-5.1.39.jar\n17/12/06 09:41:34 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0054/org.apache.commons_commons-csv-1.1.jar\n17/12/06 09:41:34 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0054/com.univocity_univocity-parsers-1.5.1.jar\n17/12/06 09:41:34 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/t_hdp/code_file/counttest5/counttest5.py\n17/12/06 09:41:34 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0054/pyspark.zip\n17/12/06 09:41:34 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0054/py4j-0.9-src.zip\n17/12/06 09:41:34 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/12/06 09:41:34 WARN Client: Resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar added multiple times to distributed cache.\n17/12/06 09:41:34 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/12/06 09:41:34 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/12/06 09:41:34 INFO Client: Uploading resource file:/tmp/spark-03976137-b717-45c3-9262-cf49ce003e69/__spark_conf__7174063951460626084.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0054/__spark_conf__7174063951460626084.zip\n17/12/06 09:41:34 INFO SecurityManager: Changing view acls to: root,hdfs\n17/12/06 09:41:34 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/12/06 09:41:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/12/06 09:41:34 INFO Client: Submitting application 54 to ResourceManager\n17/12/06 09:41:34 INFO YarnClientImpl: Submitted application application_1511598448747_0054\n17/12/06 09:41:34 INFO Client: Application report for application_1511598448747_0054 (state: ACCEPTED)\n17/12/06 09:41:34 INFO Client: \n17/12/06 09:41:34 INFO ShutdownHookManager: Shutdown hook called\n17/12/06 09:41:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-03976137-b717-45c3-9262-cf49ce003e69\n', '10', '2017-12-06 09:42:59.900115', '2017-12-06 09:41:29.249910', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('49', '21', '45', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\nmysql#mysql-connector-java added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n	found mysql#mysql-connector-java;5.1.39 in central\n:: resolution report :: resolve 215ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	mysql#mysql-connector-java;5.1.39 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 4 already retrieved (0kB/7ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1512524747688\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0055/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/12/06 09:45:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/12/06 09:45:45 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/12/06 09:45:45 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/12/06 09:45:45 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/12/06 09:45:46 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/12/06 09:45:46 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/12/06 09:45:46 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/12/06 09:45:46 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/12/06 09:45:46 INFO Client: Setting up container launch context for our AM\n17/12/06 09:45:46 INFO Client: Setting up the launch environment for our AM container\n17/12/06 09:45:46 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:45:46 INFO Client: Preparing resources for our AM container\n17/12/06 09:45:46 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:45:46 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:45:46 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0055/com.databricks_spark-csv_2.10-1.5.0.jar\n17/12/06 09:45:47 INFO Client: Uploading resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0055/mysql_mysql-connector-java-5.1.39.jar\n17/12/06 09:45:47 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0055/org.apache.commons_commons-csv-1.1.jar\n17/12/06 09:45:47 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0055/com.univocity_univocity-parsers-1.5.1.jar\n17/12/06 09:45:47 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/t_hdp/code_file/counttest4/counttest4.py\n17/12/06 09:45:47 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0055/pyspark.zip\n17/12/06 09:45:47 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0055/py4j-0.9-src.zip\n17/12/06 09:45:47 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/12/06 09:45:47 WARN Client: Resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar added multiple times to distributed cache.\n17/12/06 09:45:47 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/12/06 09:45:47 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/12/06 09:45:47 INFO Client: Uploading resource file:/tmp/spark-7347455a-c1ab-4a57-9c02-b71a59dddbf7/__spark_conf__1967005343708945562.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0055/__spark_conf__1967005343708945562.zip\n17/12/06 09:45:47 INFO SecurityManager: Changing view acls to: root,hdfs\n17/12/06 09:45:47 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/12/06 09:45:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/12/06 09:45:47 INFO Client: Submitting application 55 to ResourceManager\n17/12/06 09:45:47 INFO YarnClientImpl: Submitted application application_1511598448747_0055\n17/12/06 09:45:47 INFO Client: Application report for application_1511598448747_0055 (state: ACCEPTED)\n17/12/06 09:45:47 INFO Client: \n17/12/06 09:45:47 INFO ShutdownHookManager: Shutdown hook called\n17/12/06 09:45:47 INFO ShutdownHookManager: Deleting directory /tmp/spark-7347455a-c1ab-4a57-9c02-b71a59dddbf7\n', '10', '2017-12-06 09:46:28.055359', '2017-12-06 09:45:42.120157', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('50', '20', '46', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\nmysql#mysql-connector-java added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n	found mysql#mysql-connector-java;5.1.39 in central\n:: resolution report :: resolve 235ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	mysql#mysql-connector-java;5.1.39 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 4 already retrieved (0kB/6ms)\n	 client token: N/A\n	 diagnostics: [星期三 十二月 06 09:47:13 +0800 2017] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:202752, vCores:76> ; Queue\'s Absolute capacity = 70.0 % ; Queue\'s Absolute used capacity = 18.181818 % ; Queue\'s Absolute max capacity = 90.0 % ; \n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1512524833257\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0056/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/12/06 09:47:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/12/06 09:47:11 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/12/06 09:47:11 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/12/06 09:47:11 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/12/06 09:47:12 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/12/06 09:47:12 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/12/06 09:47:12 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/12/06 09:47:12 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/12/06 09:47:12 INFO Client: Setting up container launch context for our AM\n17/12/06 09:47:12 INFO Client: Setting up the launch environment for our AM container\n17/12/06 09:47:12 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:47:12 INFO Client: Preparing resources for our AM container\n17/12/06 09:47:12 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:47:12 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:47:12 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0056/com.databricks_spark-csv_2.10-1.5.0.jar\n17/12/06 09:47:12 INFO Client: Uploading resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0056/mysql_mysql-connector-java-5.1.39.jar\n17/12/06 09:47:12 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0056/org.apache.commons_commons-csv-1.1.jar\n17/12/06 09:47:12 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0056/com.univocity_univocity-parsers-1.5.1.jar\n17/12/06 09:47:12 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/t_hdp/code_file/counttest5/counttest5.py\n17/12/06 09:47:12 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0056/pyspark.zip\n17/12/06 09:47:12 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0056/py4j-0.9-src.zip\n17/12/06 09:47:13 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/12/06 09:47:13 WARN Client: Resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar added multiple times to distributed cache.\n17/12/06 09:47:13 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/12/06 09:47:13 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/12/06 09:47:13 INFO Client: Uploading resource file:/tmp/spark-546586c8-c2b5-45b5-ad6f-a222302368f9/__spark_conf__2839105468612627255.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0056/__spark_conf__2839105468612627255.zip\n17/12/06 09:47:13 INFO SecurityManager: Changing view acls to: root,hdfs\n17/12/06 09:47:13 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/12/06 09:47:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/12/06 09:47:13 INFO Client: Submitting application 56 to ResourceManager\n17/12/06 09:47:13 INFO YarnClientImpl: Submitted application application_1511598448747_0056\n17/12/06 09:47:13 INFO Client: Application report for application_1511598448747_0056 (state: ACCEPTED)\n17/12/06 09:47:13 INFO Client: \n17/12/06 09:47:13 INFO ShutdownHookManager: Shutdown hook called\n17/12/06 09:47:13 INFO ShutdownHookManager: Deleting directory /tmp/spark-546586c8-c2b5-45b5-ad6f-a222302368f9\n', '10', '2017-12-06 09:47:58.642106', '2017-12-06 09:47:07.758984', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('51', '20', '47', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\nmysql#mysql-connector-java added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n	found mysql#mysql-connector-java;5.1.39 in central\n:: resolution report :: resolve 216ms :: artifacts dl 6ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	mysql#mysql-connector-java;5.1.39 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 4 already retrieved (0kB/7ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1512524901499\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0057/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/12/06 09:48:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/12/06 09:48:19 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/12/06 09:48:19 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/12/06 09:48:19 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/12/06 09:48:20 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/12/06 09:48:20 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/12/06 09:48:20 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/12/06 09:48:20 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/12/06 09:48:20 INFO Client: Setting up container launch context for our AM\n17/12/06 09:48:20 INFO Client: Setting up the launch environment for our AM container\n17/12/06 09:48:20 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:48:20 INFO Client: Preparing resources for our AM container\n17/12/06 09:48:20 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:48:20 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:48:20 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0057/com.databricks_spark-csv_2.10-1.5.0.jar\n17/12/06 09:48:20 INFO Client: Uploading resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0057/mysql_mysql-connector-java-5.1.39.jar\n17/12/06 09:48:20 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0057/org.apache.commons_commons-csv-1.1.jar\n17/12/06 09:48:21 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0057/com.univocity_univocity-parsers-1.5.1.jar\n17/12/06 09:48:21 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/t_hdp/code_file/counttest5/counttest5.py\n17/12/06 09:48:21 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0057/pyspark.zip\n17/12/06 09:48:21 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0057/py4j-0.9-src.zip\n17/12/06 09:48:21 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/12/06 09:48:21 WARN Client: Resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar added multiple times to distributed cache.\n17/12/06 09:48:21 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/12/06 09:48:21 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/12/06 09:48:21 INFO Client: Uploading resource file:/tmp/spark-3158b600-b8fe-434b-8c37-b680e3e1dc6b/__spark_conf__3912449418113745994.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0057/__spark_conf__3912449418113745994.zip\n17/12/06 09:48:21 INFO SecurityManager: Changing view acls to: root,hdfs\n17/12/06 09:48:21 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/12/06 09:48:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/12/06 09:48:21 INFO Client: Submitting application 57 to ResourceManager\n17/12/06 09:48:21 INFO YarnClientImpl: Submitted application application_1511598448747_0057\n17/12/06 09:48:21 INFO Client: Application report for application_1511598448747_0057 (state: ACCEPTED)\n17/12/06 09:48:21 INFO Client: \n17/12/06 09:48:21 INFO ShutdownHookManager: Shutdown hook called\n17/12/06 09:48:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-3158b600-b8fe-434b-8c37-b680e3e1dc6b\n', '10', '2017-12-06 09:49:13.389580', '2017-12-06 09:48:16.079246', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');
INSERT INTO `execution` VALUES ('52', '20', '48', '', 'The jars for the packages stored in: /root/.ivy2/jars\n:: loading settings :: url = jar:file:/usr/hdp/2.5.3.0-37/spark/lib/spark-assembly-1.6.2.2.5.3.0-37-hadoop2.7.3.2.5.3.0-37.jar!/org/apache/ivy/core/settings/ivysettings.xml\ncom.databricks#spark-csv_2.10 added as a dependency\nmysql#mysql-connector-java added as a dependency\n:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n	confs: [default]\n	found com.databricks#spark-csv_2.10;1.5.0 in central\n	found org.apache.commons#commons-csv;1.1 in central\n	found com.univocity#univocity-parsers;1.5.1 in central\n	found mysql#mysql-connector-java;5.1.39 in central\n:: resolution report :: resolve 206ms :: artifacts dl 5ms\n	:: modules in use:\n	com.databricks#spark-csv_2.10;1.5.0 from central in [default]\n	com.univocity#univocity-parsers;1.5.1 from central in [default]\n	mysql#mysql-connector-java;5.1.39 from central in [default]\n	org.apache.commons#commons-csv;1.1 from central in [default]\n	---------------------------------------------------------------------\n	|                  |            modules            ||   artifacts   |\n	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n	---------------------------------------------------------------------\n	|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n	---------------------------------------------------------------------\n:: retrieving :: org.apache.spark#spark-submit-parent\n	confs: [default]\n	0 artifacts copied, 4 already retrieved (0kB/7ms)\n	 client token: N/A\n	 diagnostics: AM container is launched, waiting for AM container to Register with RM\n	 ApplicationMaster host: N/A\n	 ApplicationMaster RPC port: -1\n	 queue: prod\n	 start time: 1512525014933\n	 final status: UNDEFINED\n	 tracking URL: http://master.hadoop:8088/proxy/application_1511598448747_0058/\n	 user: hdfs\n\nstderr: \n\nYARN Diagnostics: \n', '17/12/06 09:50:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n17/12/06 09:50:13 INFO TimelineClientImpl: Timeline service address: http://master.hadoop:8188/ws/v1/timeline/\n17/12/06 09:50:13 INFO RMProxy: Connecting to ResourceManager at master.hadoop/10.108.211.136:8050\n17/12/06 09:50:13 INFO AHSProxy: Connecting to Application History server at master.hadoop/10.108.211.136:10200\n17/12/06 09:50:13 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n17/12/06 09:50:13 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n17/12/06 09:50:13 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (50688 MB per container)\n17/12/06 09:50:13 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n17/12/06 09:50:13 INFO Client: Setting up container launch context for our AM\n17/12/06 09:50:13 INFO Client: Setting up the launch environment for our AM container\n17/12/06 09:50:14 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:50:14 INFO Client: Preparing resources for our AM container\n17/12/06 09:50:14 INFO Client: Using the spark assembly jar on HDFS because you are using HDP, defaultSparkAssembly:hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:50:14 INFO Client: Source and destination file systems are the same. Not copying hdfs://master.hadoop:8020/hdp/apps/2.5.3.0-37/spark/spark-hdp-assembly.jar\n17/12/06 09:50:14 INFO Client: Uploading resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0058/com.databricks_spark-csv_2.10-1.5.0.jar\n17/12/06 09:50:14 INFO Client: Uploading resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0058/mysql_mysql-connector-java-5.1.39.jar\n17/12/06 09:50:14 INFO Client: Uploading resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0058/org.apache.commons_commons-csv-1.1.jar\n17/12/06 09:50:14 INFO Client: Uploading resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0058/com.univocity_univocity-parsers-1.5.1.jar\n17/12/06 09:50:14 INFO Client: Source and destination file systems are the same. Not copying hdfs://10.108.211.136:8020/user/yufeng/t_hdp/code_file/counttest5/counttest5.py\n17/12/06 09:50:14 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/pyspark.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0058/pyspark.zip\n17/12/06 09:50:14 INFO Client: Uploading resource file:/usr/hdp/2.5.3.0-37/spark/python/lib/py4j-0.9-src.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0058/py4j-0.9-src.zip\n17/12/06 09:50:14 WARN Client: Resource file:/root/.ivy2/jars/com.databricks_spark-csv_2.10-1.5.0.jar added multiple times to distributed cache.\n17/12/06 09:50:14 WARN Client: Resource file:/root/.ivy2/jars/mysql_mysql-connector-java-5.1.39.jar added multiple times to distributed cache.\n17/12/06 09:50:14 WARN Client: Resource file:/root/.ivy2/jars/org.apache.commons_commons-csv-1.1.jar added multiple times to distributed cache.\n17/12/06 09:50:14 WARN Client: Resource file:/root/.ivy2/jars/com.univocity_univocity-parsers-1.5.1.jar added multiple times to distributed cache.\n17/12/06 09:50:14 INFO Client: Uploading resource file:/tmp/spark-d6ba8442-eb10-4d64-889d-932623098ec3/__spark_conf__4181007830603304694.zip -> hdfs://master.hadoop:8020/user/hdfs/.sparkStaging/application_1511598448747_0058/__spark_conf__4181007830603304694.zip\n17/12/06 09:50:14 INFO SecurityManager: Changing view acls to: root,hdfs\n17/12/06 09:50:14 INFO SecurityManager: Changing modify acls to: root,hdfs\n17/12/06 09:50:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root, hdfs); users with modify permissions: Set(root, hdfs)\n17/12/06 09:50:14 INFO Client: Submitting application 58 to ResourceManager\n17/12/06 09:50:15 INFO YarnClientImpl: Submitted application application_1511598448747_0058\n17/12/06 09:50:15 INFO Client: Application report for application_1511598448747_0058 (state: ACCEPTED)\n17/12/06 09:50:15 INFO Client: \n17/12/06 09:50:15 INFO ShutdownHookManager: Shutdown hook called\n17/12/06 09:50:15 INFO ShutdownHookManager: Deleting directory /tmp/spark-d6ba8442-eb10-4d64-889d-932623098ec3\n', '10', '2017-12-06 09:50:55.277917', '2017-12-06 09:50:09.380421', null, '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()');

-- ----------------------------
-- Table structure for files
-- ----------------------------
DROP TABLE IF EXISTS `files`;
CREATE TABLE `files` (
  `id` int(50) NOT NULL AUTO_INCREMENT,
  `userid` int(50) DEFAULT NULL,
  `content` text,
  `deletedon` datetime(6) DEFAULT NULL,
  `modifiedon` datetime(6) DEFAULT NULL,
  `createdon` datetime(6) DEFAULT NULL,
  `isdeleted` int(5) DEFAULT '0',
  `filename` text,
  `codePath` varchar(225) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=22 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of files
-- ----------------------------
INSERT INTO `files` VALUES ('5', '4', 'import os', null, null, '2017-11-20 15:26:42.291620', '0', 'testpy1.py', '/user/yufeng/hdp/code_file/testpy1');
INSERT INTO `files` VALUES ('6', '4', 'import time\nfrom pyspark import SparkContext\nimport random\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()', null, null, '2017-11-20 22:00:27.993434', '0', 'pi.py', '/user/yufeng/hdp/code_file/pi');
INSERT INTO `files` VALUES ('7', '5', '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom operator import add\n\nif __name__ == \"__main__\":\n	if len(sys.argv) != 2:\n		print(\"Usage: counttest <file>\", file=sys.stderr)\n		exit(-1)\n\n	sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()', null, null, '2017-11-21 09:07:37.113424', '0', 'counttest.py', null);
INSERT INTO `files` VALUES ('8', '4', '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom operator import add\n\nif __name__ == \"__main__\":\n	if len(sys.argv) != 2:\n		print(\"Usage: counttest <file>\", file=sys.stderr)\n		exit(-1)\n\n	sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()', null, null, '2017-11-21 20:50:01.672424', '0', 'counttest.py', 'hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest');
INSERT INTO `files` VALUES ('9', '8', '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext\nfrom pyspark.sql import SparkSession\nfrom operator import add\n\nif __name__ == \"__main__\":\n	if len(sys.argv) != 2:\n		print(\"Usage: counttest <file>\", file=sys.stderr)\n		exit(-1)\n\n	sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()', null, null, '2017-11-21 22:03:39.512166', '0', 'counttest.py', 'hdfs://10.108.211.136:8020/user/yufeng/csj/code_file/counttest');
INSERT INTO `files` VALUES ('10', '4', '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	if len(sys.argv) != 2:\n		print(\"Usage: counttest <file>\", file=sys.stderr)\n		exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()', null, null, '2017-11-24 17:49:31.299816', '0', 'counttest2.py', 'hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest2');
INSERT INTO `files` VALUES ('11', '4', '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[0])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()', null, null, '2017-11-24 19:19:48.246693', '0', 'counttest3.py', 'hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest3');
INSERT INTO `files` VALUES ('14', '4', 'import time\nfrom pyspark import SparkContext\nimport random\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = (4.0 * count / NUM_SAMPLES)\n    sc.parallelize(str_pi).repartition(1).saveAsTextFile(\"/user/yufeng/text.txt\")\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()', null, null, '2017-11-25 17:29:25.537289', '0', 'save_pi.py', 'hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi');
INSERT INTO `files` VALUES ('15', '4', 'import time\nfrom pyspark import SparkContext\nimport random\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    List = [str_pi]\n    sc.parallelize(List).repartition(1).saveAsTextFile(\"/user/yufeng/text\")\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()', null, null, '2017-11-25 17:40:12.021997', '0', 'save_pi_v2.py', 'hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi_v2');
INSERT INTO `files` VALUES ('16', '4', 'import time\nfrom pyspark import SparkContext\nimport random\nimport sys\n\nif __name__ == \"__main__\":\n    \"\"\"\n        Usage: pi [partitions]\n    \"\"\"\n    start = time.time()\n    sc = SparkContext(appName=\"PythonPi\")\n    NUM_SAMPLES = 100000\n    def sample(p):\n      x, y = random.random(), random.random()\n      return 1 if x*x + y*y < 1 else 0\n\n    count = sc.parallelize(xrange(0, NUM_SAMPLES)).map(sample).reduce(lambda a, b: a + b)\n    str_pi = \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    List = [str_pi]\n    sc.parallelize(List).repartition(1).saveAsTextFile(sys.argv[1])\n    print \"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)\n    print \"used time = \" +str(time.time() - start)\n\n    sc.stop()', null, null, '2017-11-25 19:26:59.304539', '0', 'save_pi_v3.py', 'hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/save_pi_v3');
INSERT INTO `files` VALUES ('17', '4', '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()', null, null, '2017-11-25 20:26:46.709705', '0', 'counttest4.py', 'hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest4');
INSERT INTO `files` VALUES ('18', '4', '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()', null, null, '2017-11-25 21:33:07.212435', '0', 'counttest5.py', 'hdfs://10.108.211.136:8020/user/yufeng/hdp/code_file/counttest5');
INSERT INTO `files` VALUES ('19', '9', '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()', null, null, '2017-11-26 18:38:23.476620', '0', 'counttest5.py', 'hdfs://10.108.211.136:8020/user/yufeng/cad/code_file/counttest5');
INSERT INTO `files` VALUES ('20', '10', '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	wordcounts.repartition(1).saveAsTextFile(sys.argv[2])\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()', null, null, '2017-12-06 09:12:18.284503', '0', 'counttest5.py', 'hdfs://10.108.211.136:8020/user/yufeng/t_hdp/code_file/counttest5');
INSERT INTO `files` VALUES ('21', '10', '\"\"\"wordcount.py\"\"\"\nfrom __future__ import print_function\n\nimport sys\nfrom pyspark import SparkContext,SQLContext\nfrom operator import add\n\nif __name__ == \"__main__\":\n	# if len(sys.argv) != 2:\n	# 	print(\"Usage: counttest <file>\", file=sys.stderr)\n	# 	exit(-1)\n	sc = SparkContext(appName=\"Simple Word Count App\")\n	sqlContext = SQLContext(sc)\n	# sc = SparkContext(\"local\", \"Simple Word Count App\")\n	rdd=sc.textFile(sys.argv[1])\n	wordcounts=rdd.flatMap(lambda l: l.split(\' \')) \\\n    				.map(lambda w:(w,1)) \\\n    				.reduceByKey(lambda a,b:a+b) \\\n    				.map(lambda (a,b):(b,a)) \\\n    				.sortByKey(ascending=False)\n	output = wordcounts.collect()\n	for (count,word) in output:\n		print(\"%s: %i\" % (word,count))\n	sc.stop()', null, null, '2017-12-06 09:37:45.621158', '0', 'counttest4.py', 'hdfs://10.108.211.136:8020/user/yufeng/t_hdp/code_file/counttest4');

-- ----------------------------
-- Table structure for job
-- ----------------------------
DROP TABLE IF EXISTS `job`;
CREATE TABLE `job` (
  `jobId` int(10) NOT NULL AUTO_INCREMENT,
  `userId` int(10) NOT NULL,
  `jobName` varchar(255) DEFAULT NULL,
  `status` int(10) DEFAULT NULL,
  `property` int(10) DEFAULT '0',
  `print_log` text,
  PRIMARY KEY (`jobId`)
) ENGINE=InnoDB AUTO_INCREMENT=67 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of job
-- ----------------------------
INSERT INTO `job` VALUES ('1', '4', '测试关联度', '1', '1', 'dataPath:/home/yufeng/jsw_ml_project/data/dataset/hdp/c/丈夫属性字段.csv\nfeature size:28');
INSERT INTO `job` VALUES ('2', '4', '测试测试', '1', '1', 'dataPath:/home/yufeng/jsw_ml_project/data/dataset/hdp/c/丈夫属性字段.csv\nfeature size:28');
INSERT INTO `job` VALUES ('3', '4', '试试之后提交', '0', '1', null);
INSERT INTO `job` VALUES ('4', '4', '测试测试测试', '2', '1', null);
INSERT INTO `job` VALUES ('5', '4', '测试测', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\nDecisionTreeClassfier /home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据/churn.csv /home/yufeng/jsw_ml_project/data/model/hdp/测试测5 1 1 None None 0.2\n0.785615853007 0.0359276585386\n{\'auc_std_err\': 0.03592765853856026, \'auc_score\': 0.78561585300715742}');
INSERT INTO `job` VALUES ('6', '4', '测试决策树分类', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\nDecisionTreeClassfier /home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据/churn.csv /home/yufeng/jsw_ml_project/data/model/hdp/测试决策树分类6 1 1 None None 0.2\n0.687790478225 0.0261735926916\n{\'auc_std_err\': 0.026173592691605015, \'auc_score\': 0.68779047822526085}');
INSERT INTO `job` VALUES ('7', '4', '测试支持向量机', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\nSVM /home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据/churn.csv /home/yufeng/jsw_ml_project/data/model/hdp/测试支持向量机7 1 1 None None 0.2\n0.636278887583 0.030422565431\n{\'auc_std_err\': 0.030422565431019129, \'auc_score\': 0.63627888758323548}');
INSERT INTO `job` VALUES ('8', '4', '测试朴素贝叶斯', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\nNaiveBayes /home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据/churn.csv /home/yufeng/jsw_ml_project/data/model/hdp/测试朴素贝叶斯8 1 1 None None 0.3\n0.742849497302 0.0170394583074\n{\'auc_std_err\': 0.017039458307421127, \'auc_score\': 0.74284949730165895}');
INSERT INTO `job` VALUES ('9', '4', '测试逻辑', '3', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\nLogisticRegression /home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据/hour_noheader.csv /home/yufeng/jsw_ml_project/data/model/hdp/测试逻辑9 0 1 None None 0.3\nTraceback (most recent call last):\n  File \"/home/yufeng/jsw_ml_project/main/model/parrallel_ml/classifierRunBenchMark.py\", line 173, in <module>\n    main()\n  File \"/home/yufeng/jsw_ml_project/main/model/parrallel_ml/classifierRunBenchMark.py\", line 165, in main\n    results_dict = classifierBenchMark.run_benchmark(config, classifiers, classifiers_gridparameters)\n  File \"/home/yufeng/jsw_ml_project/main/model/parrallel_ml/classifierBenchMark.py\", line 63, in run_benchmark\n    X_train, y_train, X_test, y_test = spark_util.get_stratifed_data(X, y, float(config[\'test_size\']),config[\"class_or_regression\"])\n  File \"/home/yufeng/jsw_ml_project/main/model/parrallel_ml/spark_util.py\", line 119, in get_stratifed_data\n    sss = sklearn.cross_validation.StratifiedShuffleSplit(y,n_iter=1,test_size=test_size)\n  File \"/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py\", line 1067, in __init__\n    raise ValueError(\"The least populated class in y has only 1\"\nValueError: The least populated class in y has only 1 member, which is too few. The minimum number of labels for any class cannot be less than 2.');
INSERT INTO `job` VALUES ('10', '4', '测试逻辑', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n  % (min_groups, self.n_splits)), Warning)\nLogisticRegression /home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据/hour_noheader.csv /home/yufeng/jsw_ml_project/data/model/hdp/测试逻辑10 0 1 None None 0.2\n0.0234050432689 0.00557455213162\n{\'auc_std_err\': 0.0055745521316173767, \'auc_score\': 0.023405043268856222}');
INSERT INTO `job` VALUES ('11', '4', '测试迭代', '0', '1', null);
INSERT INTO `job` VALUES ('12', '4', '测试迭代', '0', '0', null);
INSERT INTO `job` VALUES ('13', '4', '测试迭代', '0', '0', null);
INSERT INTO `job` VALUES ('14', '4', '测试迭代', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\nGradientBoostedTreesRegression /home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据/hour_noheader.csv /home/yufeng/jsw_ml_project/data/model/hdp/测试迭代14 0 1 None None 0.3\n0.998887669125 5.43281314825e-05\n{\'auc_std_err\': 5.4328131482455334e-05, \'auc_score\': 0.99888766912501814}');
INSERT INTO `job` VALUES ('15', '4', '测试线性', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\nLinearRegression /home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据/hour_noheader.csv /home/yufeng/jsw_ml_project/data/model/hdp/测试线性15 0 1 None None 0.3\n0.999999999994 4.73143909876e-13\n{\'auc_std_err\': 4.7314390987561247e-13, \'auc_score\': 0.99999999999421829}');
INSERT INTO `job` VALUES ('16', '4', '测试linhuigui', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\nRidgeRegression /home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据/hour_noheader.csv /home/yufeng/jsw_ml_project/data/model/hdp/测试linhuigui16 0 1 None None 0.2\n0.999832566034 4.87141908537e-06\n{\'auc_std_err\': 4.8714190853675481e-06, \'auc_score\': 0.99983256603386739}');
INSERT INTO `job` VALUES ('17', '4', '反向测试回归', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\nGradientBoostedTreesRegression /home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据/hour_noheader.csv /home/yufeng/jsw_ml_project/data/model/hdp/反向测试回归17 0 1 None None 0.4\n0.998941832461 6.79184892816e-05\n{\'auc_std_err\': 6.7918489281571445e-05, \'auc_score\': 0.99894183246123824}');
INSERT INTO `job` VALUES ('18', '4', '反向测试', '0', '1', null);
INSERT INTO `job` VALUES ('19', '4', '反向测试回归回归', '3', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\nLinearRegression /home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据/hour_noheader.csv /home/yufeng/jsw_ml_project/data/model/hdp/反向测试回归回归19 1 25 None None 0.3\nTraceback (most recent call last):\n  File \"/home/yufeng/jsw_ml_project/main/model/parrallel_ml/classifierRunBenchMark.py\", line 173, in <module>\n    main()\n  File \"/home/yufeng/jsw_ml_project/main/model/parrallel_ml/classifierRunBenchMark.py\", line 165, in main\n    results_dict = classifierBenchMark.run_benchmark(config, classifiers, classifiers_gridparameters)\n  File \"/home/yufeng/jsw_ml_project/main/model/parrallel_ml/classifierBenchMark.py\", line 52, in run_benchmark\n    spark_util.scale_dataframe_features(df,config[\"label\"])\n  File \"/home/yufeng/jsw_ml_project/main/model/parrallel_ml/spark_util.py\", line 112, in scale_dataframe_features\n    num_features = [x for x in df.select_dtypes(exclude=[\'object\']).columns if x not in [df.columns[int(label)]]]\n  File \"/usr/local/miniconda2/lib/python2.7/site-packages/pandas/core/indexes/base.py\", line 1689, in __getitem__\n    return getitem(key)\nIndexError: index 25 is out of bounds for axis 0 with size 16');
INSERT INTO `job` VALUES ('20', '4', 'm', '0', '1', null);
INSERT INTO `job` VALUES ('21', '4', 'DefaultJobName2018-07-28_19:29:02', '0', '0', null);
INSERT INTO `job` VALUES ('22', '4', 'DefaultJobName2018-07-28_19:29:03', '0', '0', null);
INSERT INTO `job` VALUES ('23', '4', 'DefaultJobName2018-07-28_19:43:11', '0', '0', null);
INSERT INTO `job` VALUES ('24', '4', 'DefaultJobName2018-07-28_19:52:31', '0', '1', null);
INSERT INTO `job` VALUES ('25', '4', 'DefaultJobName2018-07-28_20:17:00', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\nDecisionTreeClassfier /home/yufeng/jsw_ml_project/data/dataset/hdp/c/丈夫属性字段.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName072820170025 1 1 None None 0.2\n0.751540564132 0.0028372994482\n{\'auc_std_err\': 0.0028372994481992838, \'auc_score\': 0.75154056413233161}');
INSERT INTO `job` VALUES ('26', '4', 'DefaultJobName20180728210243', '3', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/home/yufeng/jsw_ml_app\ndataPath:/home/yufeng/jsw_ml_project/data/dataset/hdp/c/丈夫属性字段.csv\nfeature size:28\nTraceback (most recent call last):\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/cor.py\", line 77, in <module>\n    main()\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/cor.py\", line 71, in main\n    data.to_csv(modelPath + \"/app/\" + filename,index=False)\n  File \"/usr/local/miniconda2/lib/python2.7/site-packages/pandas/core/frame.py\", line 1403, in to_csv\n    formatter.save()\n  File \"/usr/local/miniconda2/lib/python2.7/site-packages/pandas/io/formats/format.py\", line 1577, in save\n    compression=self.compression)\n  File \"/usr/local/miniconda2/lib/python2.7/site-packages/pandas/io/common.py\", line 379, in _get_handle\n    f = open(path_or_buf, mode)\nIOError: [Errno 2] No such file or directory: \'/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018072821024326/app/cor20180728210433.csv\'');
INSERT INTO `job` VALUES ('27', '4', 'DefaultJobName20180728210944', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/home/yufeng/jsw_ml_app\ndataPath:/home/yufeng/jsw_ml_project/data/dataset/hdp/c/丈夫属性字段.csv\nfeature size:28');
INSERT INTO `job` VALUES ('28', '4', '测试分类分类分类', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\nDecisionTreeClassfier /home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据/churn.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/测试分类分类分类28 1 1 None None 0.2\n0.712819855429 0.0191663724512\n{\'auc_std_err\': 0.019166372451200647, \'auc_score\': 0.71281985542855097}');
INSERT INTO `job` VALUES ('29', '4', 'DefaultJobName20180728213255', '0', '0', null);
INSERT INTO `job` VALUES ('30', '4', 'DefaultJobName20180728213301', '0', '1', null);
INSERT INTO `job` VALUES ('31', '4', '测试回归回归', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n  % (min_groups, self.n_splits)), Warning)\nLogisticRegression /home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据/hour_noheader.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/测试回归回归31 0 1 None None 0.2\n0.0187165702195 0.00184823650482\n{\'auc_std_err\': 0.0018482365048191371, \'auc_score\': 0.01871657021949933}');
INSERT INTO `job` VALUES ('32', '4', 'DefaultJobName20180728220307', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\nDecisionTreeClassfier /home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据/churn.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018072822030732 1 1 None None 0.2\n0.736774205035 0.0300829211853\n{\'auc_std_err\': 0.03008292118526671, \'auc_score\': 0.73677420503507451}');
INSERT INTO `job` VALUES ('33', '4', 'DefaultJobName20180728220512', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/model_selection/_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=5.\n  % (min_groups, self.n_splits)), Warning)\nLogisticRegression /home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据/hour_noheader.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018072822051233 0 1 None None 0.3\n0.0239982828806 0.0026432296491\n{\'auc_std_err\': 0.0026432296490997765, \'auc_score\': 0.023998282880609116}');
INSERT INTO `job` VALUES ('34', '4', 'DefaultJobName20180729191231', '3', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\nPU-learning /home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类/resample_df.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018072919123134 1 1 None None 0.2\nsize of positives: 3797 size of negatives: 7766 size of unlabeled: 11679\nTraceback (most recent call last):\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/classifierRunBenchMark.py\", line 177, in <module>\n    main()\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/classifierRunBenchMark.py\", line 169, in main\n    results_dict = classifierBenchMark.run_benchmark(config, classifiers, classifiers_gridparameters)\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/classifierBenchMark.py\", line 118, in run_benchmark\n    spark_util.all_benchmarks(clf_results, pic_path, config[\"auc_folds\"],\nNameError: global name \'pic_path\' is not defined');
INSERT INTO `job` VALUES ('35', '4', 'DefaultJobName20180729215922', '3', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\nPU-learning /home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类/resample_df.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018072921592235 1 1 None None 0.2\nTraceback (most recent call last):\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/classifierRunBenchMark.py\", line 178, in <module>\n    main()\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/classifierRunBenchMark.py\", line 170, in main\n    results_dict = classifierBenchMark.run_benchmark(config, classifiers, classifiers_gridparameters)\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/classifierBenchMark.py\", line 50, in run_benchmark\n    os.makedirs(pic_path)\n  File \"/usr/local/miniconda2/lib/python2.7/os.py\", line 157, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 13] Permission denied: \'/home/yufeng/jsw_ml_app/app/pic/35\'');
INSERT INTO `job` VALUES ('36', '4', 'DefaultJobName20180729220048', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family [u\'sans-serif\'] not found. Falling back to DejaVu Sans\n  (prop.get_family(), self.defaultFamily[fontext]))\nPU-learning /home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类/resample_df.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018072922004836 1 1 None None 0.2\nsize of positives: 3797 size of negatives: 7766 size of unlabeled: 11679 [ 1.  1.  0. ...,  0.  0.  0.] [ 1.  1.  1. ...,  0.  0.  0.]\n0.714188421766 0.00703817072379\n{\'auc_std_err\': 0.0070381707237915109, \'auc_score\': 0.71418842176557384}');
INSERT INTO `job` VALUES ('37', '4', 'DefaultJobName20181006203212', '0', '1', null);
INSERT INTO `job` VALUES ('38', '4', '关联', '0', '1', null);
INSERT INTO `job` VALUES ('39', '4', 'guanlian', '0', '1', null);
INSERT INTO `job` VALUES ('40', '4', 'guanll', '0', '0', null);
INSERT INTO `job` VALUES ('41', '4', 'guanlll', '0', '0', null);
INSERT INTO `job` VALUES ('42', '4', '关联11', '0', '0', null);
INSERT INTO `job` VALUES ('43', '4', '事实上', '0', '1', null);
INSERT INTO `job` VALUES ('44', '4', 'ddd', '0', '0', null);
INSERT INTO `job` VALUES ('45', '4', 'fff', '0', '0', null);
INSERT INTO `job` VALUES ('46', '4', 'fffff', '0', '0', null);
INSERT INTO `job` VALUES ('47', '4', 'DefaultJobName20181006231841', '0', '0', null);
INSERT INTO `job` VALUES ('48', '4', 'DefaultJobName20181006232317', '0', '0', null);
INSERT INTO `job` VALUES ('49', '4', 'DefaultJobName20181006232514', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/home/yufeng/jsw_ml_app\ndataPath:/home/yufeng/jsw_ml_project/data/dataset/hdp/c/丈夫属性字段.csv\nfeature size:28');
INSERT INTO `job` VALUES ('50', '4', 'DefaultJobName20181006233813', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family [u\'sans-serif\'] not found. Falling back to DejaVu Sans\n  (prop.get_family(), self.defaultFamily[fontext]))\nDecisionTreeClassfier /home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据/churn.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018100623381350 1 1 None None 0.2\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0\n 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0\n 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 1 1 0 0\n 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0\n 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0\n 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1\n 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0\n 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0\n 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0\n 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0\n 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1\n 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 1 0\n 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0\n 0 0 1 1 1 0 1 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0\n 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1\n 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0\n 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1\n 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0\n 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0\n 0]\n0.773841469928 0.0225693633264\n{\'auc_std_err\': 0.02256936332638863, \'auc_score\': 0.77384146992842628}');
INSERT INTO `job` VALUES ('51', '4', 'DefaultJobName20181006234120', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/home/yufeng/jsw_ml_app\ndataPath:/home/yufeng/jsw_ml_app/app/data/dataset/hdp/关联/丈夫属性字段.csv\nfeature size:28');
INSERT INTO `job` VALUES ('52', '4', 'DefaultJobName20181006234705', '0', '1', null);
INSERT INTO `job` VALUES ('53', '4', '数据分类', '3', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\nDecisionTreeClassfier /home/yufeng/jsw_ml_app/app/data/dataset/hdp/分类/hour_noheader.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/数据分类53 1 1 None None 0.2\nTraceback (most recent call last):\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/classifierRunBenchMark.py\", line 178, in <module>\n    main()\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/classifierRunBenchMark.py\", line 170, in main\n    results_dict = classifierBenchMark.run_benchmark(config, classifiers, classifiers_gridparameters)\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/classifierBenchMark.py\", line 95, in run_benchmark\n    X_train, y_train, X_test, y_test = spark_util.get_stratifed_data(X, y, float(config[\'test_size\']),config[\"class_or_regression\"])\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/spark_util.py\", line 129, in get_stratifed_data\n    sss = sklearn.cross_validation.StratifiedShuffleSplit(y,n_iter=1,test_size=test_size)\n  File \"/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py\", line 1067, in __init__\n    raise ValueError(\"The least populated class in y has only 1\"\nValueError: The least populated class in y has only 1 member, which is too few. The minimum number of labels for any class cannot be less than 2.');
INSERT INTO `job` VALUES ('55', '4', 'DefaultJobName20181007125226', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family [u\'sans-serif\'] not found. Falling back to DejaVu Sans\n  (prop.get_family(), self.defaultFamily[fontext]))\nDecisionTreeClassfier /home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据/churn.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018100712522655 1 1 None None 0.2\n[1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0\n 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0\n 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0\n 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n 0 1 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0\n 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1\n 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0\n 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0\n 0] [1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n 1 0 1 0 0 0 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0\n 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 0\n 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0\n 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0\n 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n 0 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n 0 1 1 0 0 0 1 0 1 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0\n 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1\n 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0\n 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 1 0 0\n 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n 0]\n0.687732792081 0.0285352680236\n{\'auc_std_err\': 0.028535268023552246, \'auc_score\': 0.6877327920806181}');
INSERT INTO `job` VALUES ('56', '4', '分类数据', '0', '0', null);
INSERT INTO `job` VALUES ('57', '4', '分类数据', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family [u\'sans-serif\'] not found. Falling back to DejaVu Sans\n  (prop.get_family(), self.defaultFamily[fontext]))\nDecisionTreeClassfier /home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据/churn.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/分类数据57 1 1 None None 0.2\n[0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0\n 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1\n 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1\n 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1\n 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n 0] [0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n 0 1 1 0 0 1 0 0 1 0 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 1 0 1 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0\n 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0\n 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0\n 0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 1\n 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0\n 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0 1\n 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0\n 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 1]\n0.741922515401 0.0227497973252\n{\'auc_std_err\': 0.022749797325228225, \'auc_score\': 0.74192251540077636}');
INSERT INTO `job` VALUES ('58', '4', '分类数据', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family [u\'sans-serif\'] not found. Falling back to DejaVu Sans\n  (prop.get_family(), self.defaultFamily[fontext]))\nDecisionTreeClassfier /home/yufeng/jsw_ml_project/data/dataset/hdp/分类数据/churn.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/分类数据58 1 1 None None 0.2\n[0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0\n 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0\n 1 0 0 0 0 0 0 0 0 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n 0 0 0 1 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0] [0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0\n 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0 0\n 1 0 0 0 0 0 1 0 1 1 1 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1\n 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 1 0 0\n 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 0 0\n 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1\n 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0\n 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0\n 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0\n 1 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0\n 1]\n0.74787736353 0.0333548556402\n{\'auc_std_err\': 0.033354855640172397, \'auc_score\': 0.74787736352953749}');
INSERT INTO `job` VALUES ('59', '4', '关联分析', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/home/yufeng/jsw_ml_app\ndataPath:/home/yufeng/jsw_ml_app/app/data/dataset/hdp/关联/丈夫属性字段.csv\nfeature size:28');
INSERT INTO `job` VALUES ('60', '4', '回归数据', '3', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\nLogisticRegression /home/yufeng/jsw_ml_project/data/dataset/hdp/回归数据/hour_noheader.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/回归数据60 0 1 None None 0.2\nTraceback (most recent call last):\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/classifierRunBenchMark.py\", line 178, in <module>\n    main()\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/classifierRunBenchMark.py\", line 170, in main\n    results_dict = classifierBenchMark.run_benchmark(config, classifiers, classifiers_gridparameters)\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/classifierBenchMark.py\", line 150, in run_benchmark\n    config[\'ranking_Frac\'],config[\"class_or_regression\"])\n  File \"/home/yufeng/jsw_ml_app/app/main/model/parrallel_ml/spark_util.py\", line 182, in all_benchmarks\n    y_pred = clf_results[\"y_pred\"]\nKeyError: \'y_pred\'');
INSERT INTO `job` VALUES ('64', '4', 'PU分类任务', '0', '0', null);
INSERT INTO `job` VALUES ('65', '4', 'PU分类任务', '0', '0', null);
INSERT INTO `job` VALUES ('66', '4', 'PU分类任务', '1', '1', '/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n  DeprecationWarning)\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy is deprecated, use flask_sqlalchemy instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\n/usr/local/miniconda2/lib/python2.7/site-packages/flask_sqlalchemy/__init__.py:30: ExtDeprecationWarning: Importing flask.ext.sqlalchemy._compat is deprecated, use flask_sqlalchemy._compat instead.\n  from flask.ext.sqlalchemy._compat import iteritems, itervalues, xrange, \\\nPU-learning /home/yufeng/jsw_ml_app/app/data/dataset/hdp/PU分类/resample_df.csv /home/yufeng/jsw_ml_app/app/data/model/hdp/PU分类任务66 1 1 None None 0.2\nsize of positives: 3797 size of negatives: 7766 size of unlabeled: 11679 0.714484014822 0.0134096132814\n{\'auc_std_err\': 0.013409613281396695, \'auc_score\': 0.7144840148215319}');

-- ----------------------------
-- Table structure for model
-- ----------------------------
DROP TABLE IF EXISTS `model`;
CREATE TABLE `model` (
  `id` int(10) NOT NULL AUTO_INCREMENT,
  `userId` int(10) NOT NULL,
  `jobId` int(10) NOT NULL,
  `modelName` varchar(100) NOT NULL,
  `algoName` varchar(50) NOT NULL,
  `status` int(10) NOT NULL,
  `modelPath` longtext NOT NULL,
  `time` varchar(20) DEFAULT NULL,
  `algoPara` varchar(250) DEFAULT NULL,
  `createTime` varchar(20) DEFAULT NULL,
  `datasetName` varchar(20) DEFAULT NULL,
  `Result` varchar(200) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=49 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of model
-- ----------------------------
INSERT INTO `model` VALUES ('1', '4', '1', '测试关联度', 'Cor', '1', '/home/yufeng/jsw_ml_project/data/model/hdp/测试关联度1', '83.5980949402', '[{\"name\": u\"method\", \"value\": \"kendall\"}]', '2018-07-19_18:34:50', 'c', null);
INSERT INTO `model` VALUES ('2', '4', '2', '测试测试', 'Cor', '1', '/home/yufeng/jsw_ml_project/data/model/hdp/测试测试2', '82.8865430355', '[{\"name\": u\"method\", \"value\": \"kendall\"}]', '2018-07-19_18:36:28', 'c', null);
INSERT INTO `model` VALUES ('3', '4', '3', '试试之后提交', 'DecisionTreeClassfier', '0', '', null, '[{\'name\': u\'min_samples_split\', \'value\': 2}, {\'name\': u\'max_depth\', \'value\': 5}, {\'name\': u\'min_samples_leaf\', \'value\': 1}]', null, null, null);
INSERT INTO `model` VALUES ('4', '4', '4', '测试测试测试', 'DecisionTreeClassfier', '0', '/home/yufeng/jsw_ml_project/data/model/hdp/测试测试测试4', null, '[{\"name\": u\"min_samples_split\", \"value\": 2}, {\"name\": u\"max_depth\", \"value\": 5}, {\"name\": u\"min_samples_leaf\", \"value\": 1}]', null, '分类数据', null);
INSERT INTO `model` VALUES ('5', '4', '5', '测试测', 'DecisionTreeClassfier', '1', '/home/yufeng/jsw_ml_project/data/model/hdp/测试测5', '1.83766818047', '[{\"name\": u\"min_samples_split\", \"value\": 2}, {\"name\": u\"max_depth\", \"value\": 5}, {\"name\": u\"min_samples_leaf\", \"value\": 1}]', '2018-07-19_19:28:51', '分类数据', '{\'auc_std_err\': 0.03592765853856026, \'auc_score\': 0.78561585300715742}');
INSERT INTO `model` VALUES ('6', '4', '6', '测试决策树分类', 'DecisionTreeClassfier', '1', '/home/yufeng/jsw_ml_project/data/model/hdp/测试决策树分类6', '1.95344805717', '[{\"name\": u\"min_samples_split\", \"value\": 2}, {\"name\": u\"max_depth\", \"value\": 5}, {\"name\": u\"min_samples_leaf\", \"value\": 1}]', '2018-07-19_19:31:34', '分类数据', '{\'auc_std_err\': 0.026173592691605015, \'auc_score\': 0.68779047822526085}');
INSERT INTO `model` VALUES ('7', '4', '7', '测试支持向量机', 'SVM', '1', '/home/yufeng/jsw_ml_project/data/model/hdp/测试支持向量机7', '2.27364206314', '[{\"name\": u\"kernel\", \"value\": \"linear\"}, {\"name\": u\"C\", \"value\": 1.0}, {\"name\": u\"coef0\", \"value\": 0.0}, {\"name\": u\"degree\", \"value\": 3}]', '2018-07-19_19:33:38', '分类数据', '{\'auc_std_err\': 0.030422565431019129, \'auc_score\': 0.63627888758323548}');
INSERT INTO `model` VALUES ('8', '4', '8', '测试朴素贝叶斯', 'NaiveBayes', '1', '/home/yufeng/jsw_ml_project/data/model/hdp/测试朴素贝叶斯8', '2.06870412827', '[{\"name\": u\"alpha\", \"value\": 1.0}]', '2018-07-19_19:36:09', '分类数据', '{\'auc_std_err\': 0.017039458307421127, \'auc_score\': 0.74284949730165895}');
INSERT INTO `model` VALUES ('9', '4', '9', '测试逻辑', 'LogisticRegression', '0', '/home/yufeng/jsw_ml_project/data/model/hdp/测试逻辑9', '62.4143168926', '[{\"name\": u\"C\", \"value\": 1.0}, {\"name\": u\"max_iter\", \"value\": 100}, {\"name\": u\"n_jobs\", \"value\": 1}, {\"name\": u\"tol\", \"value\": 0.0002}]', '2018-07-19_19:46:35', '回归数据', null);
INSERT INTO `model` VALUES ('10', '4', '10', '测试逻辑', 'LogisticRegression', '1', '/home/yufeng/jsw_ml_project/data/model/hdp/测试逻辑10', '62.4143168926', '[{\"name\": u\"C\", \"value\": 1.0}, {\"name\": u\"max_iter\", \"value\": 100}, {\"name\": u\"n_jobs\", \"value\": 1}, {\"name\": u\"tol\", \"value\": 0.0002}]', '2018-07-19_19:46:35', '回归数据', '{\'auc_std_err\': 0.0055745521316173767, \'auc_score\': 0.023405043268856222}');
INSERT INTO `model` VALUES ('11', '4', '14', '测试迭代', 'GradientBoostedTreesRegression', '1', '/home/yufeng/jsw_ml_project/data/model/hdp/测试迭代14', '3.96510791779', '[{\"name\": u\"n_estimators\", \"value\": 100}, {\"name\": u\"min_samples_split\", \"value\": 2}, {\"name\": u\"learning_rate\", \"value\": 0.1}, {\"name\": u\"max_depth\", \"value\": 3}, {\"name\": u\"min_samples_leaf\", \"value\": 1}]', '2018-07-19_19:55:29', '回归数据', '{\'auc_std_err\': 5.4328131482455334e-05, \'auc_score\': 0.99888766912501814}');
INSERT INTO `model` VALUES ('12', '4', '15', '测试线性', 'LinearRegression', '1', '/home/yufeng/jsw_ml_project/data/model/hdp/测试线性15', '2.21046686172', '[{\"name\": u\"n_jobs\", \"value\": 1}]', '2018-07-19_19:57:14', '回归数据', '{\'auc_std_err\': 4.7314390987561247e-13, \'auc_score\': 0.99999999999421829}');
INSERT INTO `model` VALUES ('13', '4', '16', '测试linhuigui', 'RidgeRegression', '1', '/home/yufeng/jsw_ml_project/data/model/hdp/测试linhuigui16', '1.94100189209', '[{\"name\": u\"alpha\", \"value\": 1.0}, {\"name\": u\"max_iter\", \"value\": 50}, {\"name\": u\"tol\", \"value\": 0.0002}]', '2018-07-19_19:59:38', '回归数据', '{\'auc_std_err\': 4.8714190853675481e-06, \'auc_score\': 0.99983256603386739}');
INSERT INTO `model` VALUES ('14', '4', '17', '反向测试回归', 'GradientBoostedTreesRegression', '1', '/home/yufeng/jsw_ml_project/data/model/hdp/反向测试回归17', '4.1754860878', '[{\"name\": u\"n_estimators\", \"value\": 100}, {\"name\": u\"min_samples_split\", \"value\": 2}, {\"name\": u\"learning_rate\", \"value\": 0.1}, {\"name\": u\"max_depth\", \"value\": 3}, {\"name\": u\"min_samples_leaf\", \"value\": 1}]', '2018-07-19_20:10:42', '回归数据', '{\'auc_std_err\': 6.7918489281571445e-05, \'auc_score\': 0.99894183246123824}');
INSERT INTO `model` VALUES ('15', '4', '18', '反向测试', 'GradientBoostedTreesRegression', '0', '/home/yufeng/jsw_ml_project/data/model/hdp/反向测试18', null, '[{\"name\": u\"n_estimators\", \"value\": 100}, {\"name\": u\"min_samples_split\", \"value\": 2}, {\"name\": u\"learning_rate\", \"value\": 0.1}, {\"name\": u\"max_depth\", \"value\": 3}, {\"name\": u\"min_samples_leaf\", \"value\": 1}]', null, '回归数据', null);
INSERT INTO `model` VALUES ('16', '4', '19', '反向测试回归回归', 'LinearRegression', '2', '/home/yufeng/jsw_ml_project/data/model/hdp/反向测试回归回归19', '1.70008087158', '[{\"name\": u\"n_jobs\", \"value\": 1}]', '2018-07-19_20:15:23', '回归数据', null);
INSERT INTO `model` VALUES ('17', '4', '20', 'm', 'Cor', '0', '', null, '[{\'name\': u\'method\', \'value\': \'kendall\'}]', null, null, null);
INSERT INTO `model` VALUES ('18', '4', '24', 'DefaultJobName2018-07-28_19:52:31', 'Cor', '0', '//data/model/hdp/DefaultJobName2018-07-28_19:52:3124', null, '[{\"name\": u\"method\", \"value\": \"kendall\"}]', null, 'c', null);
INSERT INTO `model` VALUES ('19', '4', '25', 'DefaultJobName2018-07-28_20:17:00', 'DecisionTreeClassfier', '2', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018-07-28_20:17:0025', '1198.4435401', '[{\"name\": u\"min_samples_split\", \"value\": 2}, {\"name\": u\"max_depth\", \"value\": 5}, {\"name\": u\"min_samples_leaf\", \"value\": 1}]', '2018-07-28_20:17:20', 'c', '{\'auc_std_err\': 0.0028372994481992838, \'auc_score\': 0.75154056413233161}');
INSERT INTO `model` VALUES ('20', '4', '26', 'DefaultJobName20180728210243', 'Cor', '2', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018072821024326', '82.4734489918', '[{\"name\": u\"method\", \"value\": \"kendall\"}]', '2018-07-28_21:03:11', 'c', null);
INSERT INTO `model` VALUES ('21', '4', '27', 'DefaultJobName20180728210944', 'Cor', '1', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018072821094427', '82.9460837841', '[{\"name\": u\"method\", \"value\": \"kendall\"}]', '2018-07-28_21:09:57', 'c', null);
INSERT INTO `model` VALUES ('22', '4', '28', '测试分类分类分类', 'DecisionTreeClassfier', '1', '/home/yufeng/jsw_ml_app/app/data/model/hdp/测试分类分类分类28', '2.23330497742', '[{\"name\": u\"min_samples_split\", \"value\": 2}, {\"name\": u\"max_depth\", \"value\": 5}, {\"name\": u\"min_samples_leaf\", \"value\": 1}]', '2018-07-28_21:32:04', '分类数据', '{\'auc_std_err\': 0.019166372451200647, \'auc_score\': 0.71281985542855097}');
INSERT INTO `model` VALUES ('23', '4', '30', 'DefaultJobName20180728213301', 'DecisionTreeClassfier', '0', '', null, '[{\'name\': u\'min_samples_split\', \'value\': 2}, {\'name\': u\'max_depth\', \'value\': 5}, {\'name\': u\'min_samples_leaf\', \'value\': 1}]', null, null, null);
INSERT INTO `model` VALUES ('24', '4', '31', '测试回归回归', 'LogisticRegression', '1', '/home/yufeng/jsw_ml_app/app/data/model/hdp/测试回归回归31', '63.2505879402', '[{\"name\": u\"C\", \"value\": 1.0}, {\"name\": u\"max_iter\", \"value\": 100}, {\"name\": u\"n_jobs\", \"value\": 1}, {\"name\": u\"tol\", \"value\": 0.0002}]', '2018-07-28_21:36:06', '回归数据', '{\'auc_std_err\': 0.0018482365048191371, \'auc_score\': 0.01871657021949933}');
INSERT INTO `model` VALUES ('25', '4', '32', 'DefaultJobName20180728220307', 'DecisionTreeClassfier', '1', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018072822030732', '2.01596212387', '[{\"name\": u\"min_samples_split\", \"value\": 2}, {\"name\": u\"max_depth\", \"value\": 5}, {\"name\": u\"min_samples_leaf\", \"value\": 1}]', '2018-07-28_22:03:29', '分类数据', '{\'auc_std_err\': 0.03008292118526671, \'auc_score\': 0.73677420503507451}');
INSERT INTO `model` VALUES ('26', '4', '33', 'DefaultJobName20180728220512', 'LogisticRegression', '1', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018072822051233', '73.8391690254', '[{\"name\": u\"C\", \"value\": 1.0}, {\"name\": u\"max_iter\", \"value\": 100}, {\"name\": u\"n_jobs\", \"value\": 1}, {\"name\": u\"tol\", \"value\": 0.0002}]', '2018-07-28_22:05:33', '回归数据', '{\'auc_std_err\': 0.0026432296490997765, \'auc_score\': 0.023998282880609116}');
INSERT INTO `model` VALUES ('27', '4', '34', 'DefaultJobName20180729191231', 'PU-learning', '2', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018072919123134', '3.18526387215', '[{\"name\":\"n_estimators\",\"value\":200},{\"name\":\"learning_rate\",\"value\":100},{\"name\":\"random_state\",\"value\":1}]', '2018-07-29_19:12:59', 'PU分类', '{\'auc_std_err\': 0.018438587756331479, \'auc_score\': 0.67890581028466723}');
INSERT INTO `model` VALUES ('28', '4', '35', 'DefaultJobName20180729215922', 'PU-learning', '2', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018072921592235', '2.00425696373', '[{\"name\": u\"n_estimators\", \"value\": 425}, {\"name\": u\"learning_rate\", \"value\": 1.2}, {\"name\": u\"random_state\", \"value\": 1}]', '2018-07-29_21:59:41', 'PU分类', null);
INSERT INTO `model` VALUES ('29', '4', '36', 'DefaultJobName20180729220048', 'PU-learning', '1', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018072922004836', '156.589541912', '[{\"name\": u\"n_estimators\", \"value\": 425}, {\"name\": u\"learning_rate\", \"value\": 1.2}, {\"name\": u\"random_state\", \"value\": 1}]', '2018-07-29_22:01:03', 'PU分类', '{\'auc_std_err\': 0.0070381707237915109, \'auc_score\': 0.71418842176557384}');
INSERT INTO `model` VALUES ('30', '4', '37', 'DefaultJobName20181006203212', 'Cor', '0', '', null, '[{\'name\': u\'method\', \'value\': \'kendall\'}]', null, null, null);
INSERT INTO `model` VALUES ('31', '4', '38', '关联', 'DecisionTreeClassfier', '0', '', null, '[{\"name\":\"min_samples_split\",\"value\":2},{\"name\":\"max_depth\",\"value\":5},{\"name\":\"min_samples_leaf\",\"value\":1}]', null, null, null);
INSERT INTO `model` VALUES ('32', '4', '39', 'guanlian', 'Cor', '0', '', null, '[{\"name\":\"method\",\"value\":\"kendall\"}]', null, null, null);
INSERT INTO `model` VALUES ('33', '4', '43', '事实上', 'Cor', '0', '', null, '[{\'name\': u\'method\', \'value\': \'kendall\'}]', null, null, null);
INSERT INTO `model` VALUES ('34', '4', '49', 'DefaultJobName20181006232514', 'Cor', '1', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018100623251449', '86.0166101456', '[{\"name\": u\"method\", \"value\": \"kendall\"}]', '2018-10-06_23:32:37', 'c', null);
INSERT INTO `model` VALUES ('35', '4', '50', 'DefaultJobName20181006233813', 'DecisionTreeClassfier', '1', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018100623381350', '6.42608594894', '[{\"name\":\"min_samples_split\",\"value\":2},{\"name\":\"max_depth\",\"value\":5},{\"name\":\"min_samples_leaf\",\"value\":1}]', '2018-10-06_23:39:38', '分类数据', '{\'auc_std_err\': 0.02256936332638863, \'auc_score\': 0.77384146992842628}');
INSERT INTO `model` VALUES ('36', '4', '51', 'DefaultJobName20181006234120', 'Cor', '1', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018100623412051', '80.4918739796', '[{\"name\": u\"method\", \"value\": \"kendall\"}]', '2018-10-06_23:41:49', '关联', null);
INSERT INTO `model` VALUES ('37', '4', '52', 'DefaultJobName20181006234705', 'Cor', '0', '', null, '[{\'name\': u\'method\', \'value\': \'kendall\'}]', null, null, null);
INSERT INTO `model` VALUES ('38', '4', '53', '数据分类', 'DecisionTreeClassfier', '2', '/home/yufeng/jsw_ml_app/app/data/model/hdp/数据分类53', '4.46014785767', '[{\"name\":\"min_samples_split\",\"value\":2},{\"name\":\"max_depth\",\"value\":\"6\"},{\"name\":\"min_samples_leaf\",\"value\":1}]', '2018-10-07_11:54:04', '分类', null);
INSERT INTO `model` VALUES ('40', '4', '55', 'DefaultJobName20181007125226', 'DecisionTreeClassfier', '1', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018100712522655', '5.51261782646', '[{\"name\":\"min_samples_split\",\"value\":2},{\"name\":\"max_depth\",\"value\":5},{\"name\":\"min_samples_leaf\",\"value\":1}]', '2018-10-07_12:52:43', '分类数据', '{\'auc_std_err\': 0.028535268023552246, \'auc_score\': 0.6877327920806181}');
INSERT INTO `model` VALUES ('41', '4', '57', '分类数据', 'DecisionTreeClassfier', '1', '/home/yufeng/jsw_ml_app/app/data/model/hdp/分类数据57', '3.59710788727', '[{\"name\":\"min_samples_split\",\"value\":2},{\"name\":\"max_depth\",\"value\":5},{\"name\":\"min_samples_leaf\",\"value\":1}]', '2018-10-07_13:04:18', '分类数据', '{\'auc_std_err\': 0.022749797325228225, \'auc_score\': 0.74192251540077636}');
INSERT INTO `model` VALUES ('42', '4', '58', '分类数据', 'DecisionTreeClassfier', '1', '/home/yufeng/jsw_ml_app/app/data/model/hdp/分类数据58', '3.59710788727', '[{\"name\":\"min_samples_split\",\"value\":2},{\"name\":\"max_depth\",\"value\":\"6\"},{\"name\":\"min_samples_leaf\",\"value\":1}]', '2018-10-07_13:04:18', '分类数据', '{\'auc_std_err\': 0.033354855640172397, \'auc_score\': 0.74787736352953749}');
INSERT INTO `model` VALUES ('43', '4', '59', '关联分析', 'Cor', '1', '/home/yufeng/jsw_ml_app/app/data/model/hdp/关联分析59', '170.162464857', '[{\"name\":\"method\",\"value\":\"kendall\"}]', '2018-10-07_13:08:24', '关联', null);
INSERT INTO `model` VALUES ('44', '4', '60', '回归数据', 'LogisticRegression', '2', '/home/yufeng/jsw_ml_app/app/data/model/hdp/回归数据60', '90.8821051121', '[{\"name\":\"C\",\"value\":1},{\"name\":\"max_iter\",\"value\":100},{\"name\":\"n_jobs\",\"value\":\"2\"},{\"name\":\"tol\",\"value\":0.0002}]', '2018-10-07_13:13:44', '回归数据', null);
INSERT INTO `model` VALUES ('48', '4', '66', 'PU分类任务', 'PU-learning', '1', '/home/yufeng/jsw_ml_app/app/data/model/hdp/PU分类任务66', '156.734931946', '[{\"name\":\"n_estimators\",\"value\":425},{\"name\":\"learning_rate\",\"value\":1.2},{\"name\":\"random_state\",\"value\":1}]', '2018-11-22_10:19:12', 'PU分类', '{\'auc_std_err\': 0.013409613281396695, \'auc_score\': 0.7144840148215319}');

-- ----------------------------
-- Table structure for result
-- ----------------------------
DROP TABLE IF EXISTS `result`;
CREATE TABLE `result` (
  `id` int(50) NOT NULL AUTO_INCREMENT,
  `userId` int(50) DEFAULT NULL,
  `jobId` int(50) DEFAULT NULL,
  `filename` varchar(200) DEFAULT NULL,
  `createdon` datetime(6) DEFAULT NULL,
  `path` varchar(100) DEFAULT NULL,
  `status` int(20) DEFAULT '1',
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of result
-- ----------------------------
INSERT INTO `result` VALUES ('1', '4', '1', 'cor20180719183614.csv', '2018-07-19 18:36:14.241985', '/home/yufeng/jsw_ml_project/data/model/hdp/测试关联度1', '0');
INSERT INTO `result` VALUES ('2', '4', '2', 'cor20180719183750.csv', '2018-07-19 18:37:50.775567', '/home/yufeng/jsw_ml_project/data/model/hdp/测试测试2', '0');
INSERT INTO `result` VALUES ('3', '4', '27', 'cor20180728211120.csv', '2018-07-28 21:11:20.204687', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018072821094427', '0');
INSERT INTO `result` VALUES ('4', '4', '49', 'cor20181006233402.csv', '2018-10-06 23:34:02.910185', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018100623251449', '0');
INSERT INTO `result` VALUES ('5', '4', '51', 'cor20181006234310.csv', '2018-10-06 23:43:10.295643', '/home/yufeng/jsw_ml_app/app/data/model/hdp/DefaultJobName2018100623412051', '0');
INSERT INTO `result` VALUES ('6', '4', '59', 'cor20181007131114.csv', '2018-10-07 13:11:14.096779', '/home/yufeng/jsw_ml_app/app/data/model/hdp/关联分析59', '0');

-- ----------------------------
-- Table structure for user
-- ----------------------------
DROP TABLE IF EXISTS `user`;
CREATE TABLE `user` (
  `userId` int(10) NOT NULL AUTO_INCREMENT,
  `userName` varchar(20) NOT NULL,
  `password` varchar(20) NOT NULL,
  PRIMARY KEY (`userId`),
  UNIQUE KEY `userName` (`userName`)
) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8;

-- ----------------------------
-- Records of user
-- ----------------------------
INSERT INTO `user` VALUES ('1', 'test', 'test');
INSERT INTO `user` VALUES ('2', 's', 'sf');
INSERT INTO `user` VALUES ('3', 'test2', 'test');
INSERT INTO `user` VALUES ('4', 'hdp', 'test');
INSERT INTO `user` VALUES ('5', 'zcc', 'test');
INSERT INTO `user` VALUES ('6', 'yufeng', 'test');
INSERT INTO `user` VALUES ('7', 'lhh', 'test');
INSERT INTO `user` VALUES ('8', 'csj', 'test');
INSERT INTO `user` VALUES ('9', 'cad', 'test');
INSERT INTO `user` VALUES ('10', 't_hdp', 'test');
